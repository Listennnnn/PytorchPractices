{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.009702Z",
     "start_time": "2024-05-20T02:17:02.435483Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 张量创建",
   "id": "c713a88f2d09d904"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 从Python列表或Numpy数组创建张量",
   "id": "26a59b1a94449ae3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.025655Z",
     "start_time": "2024-05-20T02:17:05.011692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 从列表创建张量\n",
    "list_data = [1, 2, 3, 4]\n",
    "tensor_from_list = torch.tensor(list_data)\n",
    "print(tensor_from_list) # tensor([1, 2, 3, 4])"
   ],
   "id": "8343f61aac96e23c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.041470Z",
     "start_time": "2024-05-20T02:17:05.026653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 从NumPy数组创建张量\n",
    "np_array = np.array([1, 2, 3])\n",
    "tensor_from_np_tensor = torch.tensor(np_array)\n",
    "print(tensor_from_np_tensor) # tensor([1, 2, 3], dtype=torch.int32)"
   ],
   "id": "52d3a76953dfea1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用固定数值创建张量",
   "id": "52085cf970faa09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.056783Z",
     "start_time": "2024-05-20T02:17:05.043465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.zeros(shape)和torch.ones(shape)：创建指定形状的全零或全一张量\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "print(zeros_tensor)\n",
    "ones_tensor = torch.ones(3, 4)\n",
    "print(ones_tensor)\n",
    "\n",
    "# torch.rand(shape)：创建指定形状的随机浮点数张量（0到1之间）或标准正态分布的张量\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "print(rand_tensor)\n",
    "normal_tensor = torch.randn(3, 4)\n",
    "print(normal_tensor)\n",
    "\n",
    "# torch.arange(start, end=None, step=1, dtype=None, layout=torch.strided, device=None, requires_grad=False)：创建一个等差序列张量，默认step为1\n",
    "arange_tensor = torch.arange(1, 10, 2)\n",
    "print(arange_tensor)"
   ],
   "id": "31e528359a12bd8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0.7361, 0.2133, 0.7184],\n",
      "        [0.6451, 0.5046, 0.8740]])\n",
      "tensor([[-0.8798, -0.2917, -0.3025, -0.0895],\n",
      "        [-0.7922, -1.0611, -1.1565,  0.8130],\n",
      "        [ 0.7061,  0.7567, -0.6330, -0.0865]])\n",
      "tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 张量的基本操作",
   "id": "8051eedd4672d756"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 张量索引和切片",
   "id": "c116ef18f4e0715f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.072740Z",
     "start_time": "2024-05-20T02:17:05.057779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 张量的索引和切片类似于Python列表，可以访问和修改张量的特定元素或子集\n",
    "# 创建一个二维张量\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 索引单个元素\n",
    "element = tensor[0, 0]  # 1\n",
    "\n",
    "# 切片\n",
    "slice_1 = tensor[0, :]  # [1, 2, 3]\n",
    "slice_2 = tensor[:, 1]  # [2, 5]\n",
    "\n",
    "# 修改元素\n",
    "tensor[0, 0] = 7 # [[7, 2, 3], [4, 5, 6]]"
   ],
   "id": "a77f3d44574aa61e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 形状操作",
   "id": "ccc484bedbb9ade1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.087699Z",
     "start_time": "2024-05-20T02:17:05.074734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shape属性获取张量的形状\n",
    "shape = tensor.shape  # torch.Size([2, 3])\n",
    "print(shape)"
   ],
   "id": "8683886133d0ba50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.103657Z",
     "start_time": "2024-05-20T02:17:05.089695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# unsqueeze(dim)在指定维度添加一个大小为1的新维度\n",
    "unsqueeze_tensor = tensor.unsqueeze(0) # 添加一个新维度\n",
    "print(unsqueeze_tensor)"
   ],
   "id": "a108492d27c498b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7, 2, 3],\n",
      "         [4, 5, 6]]])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.119614Z",
     "start_time": "2024-05-20T02:17:05.104655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transpose(dim0, dim1)交换指定的两个维度\n",
    "transposed_tensor = tensor.transpose(0, 1) # 交换第一和第二维度\n",
    "print(transposed_tensor)\n",
    "# reshape(shape)或view(shape)改变张量的形状\n",
    "reshaped_tensor = tensor.reshape(6)  # 变为形状为[6]的一维张量\n",
    "print(reshaped_tensor)"
   ],
   "id": "cdf490083dde0bc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([7, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数学运算",
   "id": "354047b1b3cb79a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.135106Z",
     "start_time": "2024-05-20T02:17:05.120611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PyTorch张量支持广泛的数学运算，包括基本的算术运算、元素级运算、矩阵运算等。以tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])为例\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# 基本运算（适用于标量、一维或多维张量）\n",
    "add_tensor = tensor + 1 # [[2, 3, 4], [5, 6, 7]]\n",
    "sub_tensor = tensor - 1 # [[0, 1, 2], [3, 4, 5]]\n",
    "mul_tensor = tensor * 2 # [[2, 4, 6], [8, 10, 12]]\n",
    "div_tensor = tensor / 2 # [[0.5, 1.0, 1.5], [2.0, 2.5, 3.0]]\n",
    "addition = tensor + tensor # [[2, 4, 6], [8, 10, 12]]。\n",
    "subtraction = tensor - tensor #[[0, 0, 0], [0, 0, 0]]\n",
    "multiplication = tensor * tensor #[[1, 4, 9], [16, 25, 36]]\n",
    "division = tensor / tensor #[[1, 1, 1], [1, 1, 1]]\n"
   ],
   "id": "4802f64a067e16a8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.151070Z",
     "start_time": "2024-05-20T02:17:05.138098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 元素级运算（*运算符在这种情况下表示逐元素乘法，而不是矩阵乘法）\n",
    "element_wise_mul_tensor = tensor * tensor # [[1, 4, 9], [16, 25, 36]]"
   ],
   "id": "66ac71c1da44fcaa",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.166023Z",
     "start_time": "2024-05-20T02:17:05.154057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 矩阵运算（使用@运算符或torch.matmul()）\n",
    "matmul_tensor = tensor @ tensor.t() # 或者 torch.matmul(tensor, tensor.t())\n",
    "print(matmul_tensor)"
   ],
   "id": "761c2d6842065052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.180983Z",
     "start_time": "2024-05-20T02:17:05.168017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 广播机制（使得不同形状的张量能够进行运算）\n",
    "\"\"\"\n",
    "  结果是一个2x3的张量，其中每个元素都是tensor对应位置的元素加上1\n",
    "  broadcasted_addition = [[2, 3, 4], [5, 6, 7]]\n",
    "  broadcasted_addition = tensor + torch.tensor([1, 1, 1])\n",
    "\"\"\"\n",
    "broadcasted_addition = tensor + torch.tensor([1, 1, 1])\n",
    "print(broadcasted_addition)"
   ],
   "id": "7d9ff8ea2f1ee95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.196940Z",
     "start_time": "2024-05-20T02:17:05.181981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 比较运算（返回布尔张量）\n",
    "# 返回一个2x3的布尔张量，所有元素都为False，因为每个元素都不大于自身。\n",
    "greater_than = tensor > tensor\n",
    "print(greater_than)"
   ],
   "id": "aea6517ba121d54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.212934Z",
     "start_time": "2024-05-20T02:17:05.197938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 聚合运算（如求和、平均、最大值、最小值等）\n",
    "sum = torch.sum(tensor) # 返回张量所有元素的总和，即1+2+3+4+5+6=21。\n",
    "print(sum)\n",
    "mean = torch.mean(tensor.float())  # 返回张量的平均值，即21/6=3.5\n",
    "print(mean)\n",
    "# 返回最大值张量[4, 5, 6]和最大值的索引[1, 1, 1]，因为第二行的所有元素都是最大值。\n",
    "max_value, max_index = torch.max(tensor, dim=0)  # 按列求最大值和对应索引\n",
    "print(max_value)\n",
    "print(max_index)"
   ],
   "id": "cc74291aa81462b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21)\n",
      "tensor(3.5000)\n",
      "tensor([4, 5, 6])\n",
      "tensor([1, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 自动求导",
   "id": "7b05d00a30eea5c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 张量的requires_grad属性",
   "id": "14ee92c282c47ea8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.228855Z",
     "start_time": "2024-05-20T02:17:05.213895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在深度学习中，自动求导（automatic differentiation）是关键步骤，用于计算损失函数对模型参数的梯度。PyTorch提供了自动求导机制，可以轻松地定义和计算复杂的神经网络。\n",
    "# 创建一个张量并设置requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "# Original tensor: tensor([1., 2.], requires_grad=True)\n",
    "print(\"Original tensor:\", x)"
   ],
   "id": "669503386a52b2bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([1., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 张量操作与计算图",
   "id": "7e3991ccab678eac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.244812Z",
     "start_time": "2024-05-20T02:17:05.229852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中，张量操作与计算图紧密相关，因为计算图是实现自动求导（autograd）的关键。当一个张量的requires_grad属性被设置为True时，PyTorch会记录对该张量的所有操作，形成一个计算图。这个图描述了从输入张量到输出张量的计算路径，每个节点代表一个张量，边代表操作。\n",
    "\n",
    "# 创建一个需要梯度的张量\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# 张量操作\n",
    "y = x + 2       # 加法操作\n",
    "z = y * y * 3   # 乘法操作\n",
    "out = z.mean()  # 平均值操作\n",
    "\n",
    "#此时，计算图已经隐含地构建完成，记录了从x到out的所有操作。其中，x是计算图的起点，out是终点。"
   ],
   "id": "f9459c2675b8ad19",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 计算梯度",
   "id": "210f69e5e95acf1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.306741Z",
     "start_time": "2024-05-20T02:17:05.245809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 要计算梯度，只需对计算图的最终输出调用.backward()方法。\n",
    "out.backward()\n",
    "\n",
    "# 现在，x的梯度已经计算出来了，可以通过x.grad属性获取\n",
    "print(\"Gradient of x with respect to the output:\", x.grad)\n",
    "\n",
    "#out.backward()会沿着计算图反向传播，计算所有涉及张量的梯度。在本例中，x.grad将会给出x相对于out的梯度，即out关于x的偏导数。这在训练神经网络时非常有用，因为可以用来更新网络的权重。"
   ],
   "id": "4e4294af8ea2f187",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x with respect to the output: tensor([ 9., 12.])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 阻止梯度追踪",
   "id": "b3b9a5a2014a60f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.322697Z",
     "start_time": "2024-05-20T02:17:05.307739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在某些场景下，比如验证模型或计算某些不需要更新参数的中间结果时，阻止梯度追踪可以减少内存消耗和提高效率。使用.detach()或torch.no_grad()是实现这一目的的有效手段。\n",
    "original_tensor = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "#使用.detach()方法: 返回一个与原始张量数值相同的新张量，但不跟踪梯度。\n",
    "new_tensor = original_tensor.detach()\n",
    "print(\"New tensor:\", new_tensor)\n",
    "#使用torch.no_grad()上下文管理器。\n",
    "# with torch.no_grad():\n",
    "    # 在此区域内，所有张量的梯度将不会被追踪\n",
    "    # intermediate_result = some_operation(original_tensor)"
   ],
   "id": "99d56a1d6c4b3877",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tensor: tensor([1., 2.])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 控制梯度计算的上下文管理器",
   "id": "919860cb6b5a16d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:05.338656Z",
     "start_time": "2024-05-20T02:17:05.323695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.autograd.set_grad_enabled(True|False) 是另一个强大的工具，用于全局控制是否在代码的特定部分进行梯度计算。相比.detach()和torch.no_grad()，它提供了更多的灵活性，因为它允许在代码的不同部分动态开启或关闭梯度追踪，这对于复杂的模型调试、性能优化或混合精度训练等场景特别有用。\n",
    "\n",
    "# 默认情况下，梯度追踪是开启的\n",
    "print(f\"当前梯度追踪状态: {torch.is_grad_enabled()}\")  # 输出: True\n",
    "\n",
    "# 使用set_grad_enabled(False)关闭梯度追踪\n",
    "with torch.autograd.set_grad_enabled(False):\n",
    "    x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "    y = x * 2\n",
    "    print(f\"在上下文中，梯度追踪状态: {torch.is_grad_enabled()}\")  # 输出: False\n",
    "    print(f\"y的requires_grad属性: {y.requires_grad}\")  # 输出: False\n",
    "\n",
    "# 离开上下文后，梯度追踪状态恢复到之前的状态\n",
    "print(f\"离开上下文后，梯度追踪状态: {torch.is_grad_enabled()}\")  # 输出: True\n",
    "\n",
    "# 这里x的梯度追踪仍然是开启的，除非在其他地方被改变"
   ],
   "id": "dec0be7fa036d874",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前梯度追踪状态: True\n",
      "在上下文中，梯度追踪状态: False\n",
      "y的requires_grad属性: False\n",
      "离开上下文后，梯度追踪状态: True\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 优化器与自动求导结合",
   "id": "a695fd99a78fc080"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:06.901878Z",
     "start_time": "2024-05-20T02:17:05.339654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在训练神经网络模型时，通常会使用优化器（optimizer）来更新模型的参数。优化器利用自动求导计算出的梯度来调整参数，以最小化损失函数。以下是一个使用随机梯度下降（SGD）优化器的简单示例：\n",
    "\n",
    "# 定义一个简单的线性模型\n",
    "model = nn.Linear(2, 1)  # 输入2个特征，输出1个值\n",
    "\n",
    "# 假设有一些输入数据和标签\n",
    "inputs = torch.randn(10, 2)  # 10个样本，每个样本2个特征\n",
    "labels = torch.randn(10, 1)  # 10个样本，每个样本1个标签\n",
    "\n",
    "# 创建优化器，指定要更新的模型参数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 学习率为0.01\n",
    "\n",
    "# 前向传播\n",
    "outputs = model(inputs)\n",
    "\n",
    "# 损失函数\n",
    "loss = nn.MSELoss()(outputs, labels)\n",
    "\n",
    "# 反向传播并计算梯度\n",
    "loss.backward()\n",
    "\n",
    "# 使用优化器更新参数\n",
    "optimizer.step()\n",
    "\n",
    "# 清除梯度（防止梯度累积）\n",
    "optimizer.zero_grad()"
   ],
   "id": "b07302cfc7d0cc72",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 神经网络层",
   "id": "f2e2e2650e920bc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 创建自定义神经网络层",
   "id": "7d5c98db885b037d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:06.917835Z",
     "start_time": "2024-05-20T02:17:06.902876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中，nn.Module是构建神经网络模型的核心类，它提供了一个模块化的框架，可以方便地组合各种层和操作。\n",
    "\n",
    "# 创建自定义神经网络层是PyTorch中常见的做法。以下是如何创建一个名为CustomLayer的自定义层，它包含一个线性层和一个激活函数（例如ReLU）\n",
    "# 其中，CustomLayer类继承自nn.Module，并在__init__方法中定义了两个线性层（一个输入层和一个输出层）以及一个ReLU激活函数。forward方法描述了输入到输出的计算流程：首先，输入通过线性层，然后通过ReLU激活，最后通过输出线性层。如果只需要一个线性变换和激活，可以去掉output_linear。\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CustomLayer, self).__init__()\n",
    "\n",
    "        # 创建线性层\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # 创建ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 创建输出线性层（如果需要的话，例如对于分类任务）\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用线性变换\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # 应用ReLU激活函数\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 如果需要，可以添加更多的操作，例如另一个线性层\n",
    "        x = self.output_linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 使用这个自定义层，可以像使用内置层一样在模型中实例化和使用它：\n",
    "# input_size = 10\n",
    "# hidden_size = 20\n",
    "# output_size = 5\n",
    "# model = CustomLayer(input_size, hidden_size, output_size)"
   ],
   "id": "d61393ddb214e759",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建复杂的神经网络模型",
   "id": "6a5cdb749267d375"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:06.932796Z",
     "start_time": "2024-05-20T02:17:06.918834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构建复杂的神经网络模型通常涉及到将多个基本层或者自定义层按照特定顺序组合起来。以下是一个使用自定义SimpleLayer类来构建多层感知机（MLP）的示例\n",
    "\n",
    "class SimpleLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # 第一层：输入层到隐藏层\n",
    "        self.layer1 = SimpleLayer(input_dim, hidden_dim)\n",
    "        \n",
    "        # 第二层：隐藏层到输出层\n",
    "        self.layer2 = SimpleLayer(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)  # 第一层前向传播\n",
    "        x = self.layer2(x)  # 第二层前向传播\n",
    "        return x\n",
    "\n",
    "# 实例化一个MLP模型\n",
    "input_dim = 784  # 假设输入维度为784（例如，MNIST数据集）\n",
    "hidden_dim = 128  # 隐藏层维度\n",
    "output_dim = 10  # 输出维度（例如，10类分类问题）\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 在以上代码中，MLP类定义了一个包含两个SimpleLayer实例的多层感知机。第一个SimpleLayer接收输入数据并转换到隐藏层空间，第二个SimpleLayer则负责从隐藏层空间映射到输出层。通过这种方式，可以灵活地堆叠多个层来构造复杂的神经网络模型，每增加一层，模型的表达能力就可能增强，从而能学习到更复杂的输入-输出映射关系。"
   ],
   "id": "310f846785496115",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer1): SimpleLayer(\n",
      "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (layer2): SimpleLayer(\n",
      "    (linear): Linear(in_features=128, out_features=10, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模块的嵌套和子模块",
   "id": "f149a63132a0a588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:06.947755Z",
     "start_time": "2024-05-20T02:17:06.933794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中，nn.Module的嵌套和子模块是构建复杂神经网络架构的关键。下面是一个名为ComplexModel的示例，它包含了两个子模块：一个CustomLayer和一个MLP：\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ComplexModel, self).__init__()\n",
    "\n",
    "        # 创建一个CustomLayer实例\n",
    "        self.custom_layer = CustomLayer(input_size, hidden_size, output_size)\n",
    "\n",
    "        # 创建一个MLP实例\n",
    "        self.mlp = MLP(hidden_size, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.custom_layer(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "# 实例化一个ComplexModel\n",
    "input_size = 784  # 假设输入维度为784\n",
    "hidden_size = 128  # 隐藏层维度\n",
    "output_size = 10  # 输出维度\n",
    "model = ComplexModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 其中，ComplexModel类包含两个子模块：custom_layer和mlp。custom_layer是一个自定义层，而mlp是一个多层感知机。当在ComplexModel的forward方法中调用这些子模块时，它们的计算将按顺序进行，并且所有子模块的参数和梯度都会被自动跟踪。这种模块化的方法使得代码易于理解和维护，同时可以方便地重用和组合现有的层和模型。"
   ],
   "id": "52519cbe55df1574",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexModel(\n",
      "  (custom_layer): CustomLayer(\n",
      "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (output_linear): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (layer1): SimpleLayer(\n",
      "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (layer2): SimpleLayer(\n",
      "      (linear): Linear(in_features=128, out_features=10, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 访问模块的参数",
   "id": "67974b38b8d86d9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:06.978673Z",
     "start_time": "2024-05-20T02:17:06.948775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中，nn.Module类提供了两种方法来访问模型的参数：parameters()和named_parameters()。这两个方法都可以用来遍历模型的所有参数，但它们的区别在于返回的内容。\n",
    "\n",
    "# parameters()方法： 返回一个可迭代的生成器，其中每个元素是一个张量，代表模型的一个参数。\n",
    "model = ComplexModel(10,10,10)\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    \n",
    "# named_parameters()方法： 返回一个可迭代的生成器，其中每个元素是一个元组，包含参数的名称和对应的张量\n",
    "model = ComplexModel(10,10,10)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}, Parameter: {param}\")\n",
    "\n",
    "# 在实际应用中，通常使用named_parameters()方法，因为这样可以同时获取参数的名称，这对于调试和可视化模型参数很有用。"
   ],
   "id": "1397bfefd9b5d237",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0041, -0.0448,  0.1374,  0.2816,  0.2255,  0.2984, -0.1832,  0.0821,\n",
      "         -0.2938,  0.1101],\n",
      "        [-0.1888,  0.1946,  0.1010,  0.0429,  0.0397, -0.1405, -0.0963, -0.2811,\n",
      "          0.2248, -0.2841],\n",
      "        [-0.2252, -0.0056, -0.1626, -0.0043,  0.2055,  0.0598,  0.0379,  0.2281,\n",
      "         -0.2899, -0.1352],\n",
      "        [-0.0177,  0.0516, -0.0468, -0.0523, -0.3075, -0.0569, -0.0120, -0.0947,\n",
      "          0.1818,  0.1609],\n",
      "        [-0.2833,  0.2008, -0.3032, -0.2968, -0.2206,  0.3019,  0.1044, -0.1175,\n",
      "          0.0905, -0.0068],\n",
      "        [ 0.2953, -0.0691,  0.1397,  0.1792,  0.0667, -0.2845,  0.0106, -0.2031,\n",
      "         -0.1681, -0.0558],\n",
      "        [-0.0576,  0.2561,  0.2407,  0.1671,  0.2962, -0.3123, -0.0089, -0.2427,\n",
      "         -0.0379, -0.0666],\n",
      "        [-0.0195,  0.2494, -0.2667,  0.3024, -0.0455, -0.0649,  0.1940,  0.1701,\n",
      "         -0.0446, -0.1529],\n",
      "        [-0.2859, -0.1186, -0.2056,  0.0537,  0.0658, -0.2001,  0.0684, -0.2040,\n",
      "         -0.0476, -0.1479],\n",
      "        [ 0.0696,  0.1249,  0.1527, -0.0225, -0.0147, -0.1339, -0.2818, -0.0059,\n",
      "          0.0677,  0.1751]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3073,  0.0826,  0.0047,  0.2825, -0.2033, -0.0476, -0.0143,  0.0512,\n",
      "         0.1184,  0.1295], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.8728e-01,  2.3565e-01,  2.3874e-01,  2.5825e-01, -1.5675e-01,\n",
      "         -1.1448e-01,  1.4020e-01, -1.8898e-01, -1.6640e-01, -4.5516e-02],\n",
      "        [-1.0706e-01,  8.3393e-02, -2.2841e-01,  2.3343e-01,  9.0874e-02,\n",
      "          2.7695e-01,  2.4317e-01,  1.5430e-01,  4.1651e-02,  1.6538e-01],\n",
      "        [-2.0020e-01,  5.7624e-02,  1.3375e-01, -9.0367e-02,  5.9698e-02,\n",
      "         -1.0288e-01, -1.6503e-01,  4.1576e-02,  1.7779e-01,  2.1117e-01],\n",
      "        [ 1.9880e-01, -2.5349e-01, -1.1376e-01,  5.4425e-03, -2.2567e-01,\n",
      "         -1.6166e-01,  1.0595e-01,  2.9467e-01,  2.0936e-01,  1.3961e-01],\n",
      "        [ 2.6297e-01, -2.4619e-01,  1.2021e-01, -1.6899e-01, -5.8569e-02,\n",
      "          2.2047e-01, -7.1355e-02, -2.5478e-01,  1.7351e-01, -2.9068e-01],\n",
      "        [-2.6947e-01,  6.6535e-02,  4.9757e-03, -2.5923e-01, -1.8792e-01,\n",
      "         -5.6993e-02, -1.1178e-01,  3.6615e-04,  4.4082e-02,  1.0468e-01],\n",
      "        [-2.3885e-01, -2.1582e-02, -2.8895e-01,  1.3452e-01, -4.1020e-02,\n",
      "         -5.9456e-02, -1.8844e-04, -1.0792e-01, -3.9348e-02,  1.2843e-01],\n",
      "        [-1.5398e-01,  2.3284e-01,  9.0205e-02,  1.9574e-02,  2.9407e-01,\n",
      "         -2.3493e-02, -2.1612e-01, -2.5655e-01,  1.2039e-01,  3.9109e-03],\n",
      "        [-1.9687e-02,  2.8708e-01, -2.1734e-01, -2.8629e-01, -1.7291e-01,\n",
      "          1.0022e-01, -2.6279e-01,  8.9174e-02,  1.6964e-01,  2.2620e-01],\n",
      "        [ 9.7179e-02,  2.2976e-02, -1.4583e-01,  2.9094e-02, -7.4492e-03,\n",
      "         -4.9151e-02,  2.2166e-01,  9.1300e-02, -1.8616e-01,  1.0706e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1915,  0.0028,  0.1788, -0.2068,  0.1069, -0.0669, -0.1765, -0.2969,\n",
      "        -0.2194, -0.2853], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1986, -0.1000,  0.1952,  0.0322, -0.1154,  0.2126,  0.0888, -0.2350,\n",
      "          0.0280,  0.1599],\n",
      "        [-0.1838,  0.1973, -0.1345, -0.1481, -0.0158, -0.1796,  0.0361, -0.0010,\n",
      "         -0.0684,  0.2079],\n",
      "        [ 0.1982, -0.0244,  0.1920, -0.0990,  0.2290,  0.0661,  0.0854,  0.2852,\n",
      "          0.2191, -0.2746],\n",
      "        [-0.2921,  0.1996,  0.0172, -0.0154, -0.0105,  0.1400,  0.1924, -0.2406,\n",
      "          0.0368, -0.2824],\n",
      "        [ 0.2131,  0.3103, -0.0740, -0.0652, -0.1067, -0.2753, -0.1193,  0.2792,\n",
      "          0.3102, -0.2731],\n",
      "        [ 0.1108, -0.2352,  0.0139,  0.1805,  0.2262,  0.1115, -0.3149, -0.2240,\n",
      "         -0.0150, -0.2687],\n",
      "        [-0.0133, -0.1109,  0.1616, -0.1794,  0.0705,  0.0759,  0.0774, -0.0842,\n",
      "         -0.1417,  0.0293],\n",
      "        [-0.2593,  0.1101,  0.2584,  0.0114,  0.0628,  0.1832,  0.1132,  0.1903,\n",
      "         -0.1299, -0.0266],\n",
      "        [-0.2014,  0.0618, -0.0960,  0.0689, -0.1364,  0.2738,  0.0789,  0.1408,\n",
      "          0.2490,  0.1903],\n",
      "        [-0.0746, -0.0524, -0.1792,  0.1920, -0.0891, -0.1116, -0.2940,  0.2264,\n",
      "          0.1005, -0.1210]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1433, -0.1326,  0.0296, -0.2026,  0.2649, -0.1774,  0.0473, -0.0179,\n",
      "        -0.1149,  0.2675], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3148,  0.1345, -0.0177, -0.1305, -0.2745, -0.1820, -0.2631, -0.1569,\n",
      "          0.1442,  0.2650],\n",
      "        [ 0.0075, -0.2977,  0.0655, -0.2074,  0.0463,  0.2380,  0.2871, -0.1517,\n",
      "          0.0073, -0.0289],\n",
      "        [-0.0538,  0.1891, -0.0480, -0.1028,  0.2186,  0.2757,  0.2009, -0.0422,\n",
      "          0.0359,  0.0249],\n",
      "        [ 0.2883,  0.2817,  0.0261, -0.2468, -0.0015,  0.2939,  0.2031,  0.0288,\n",
      "          0.1942, -0.0908],\n",
      "        [-0.2859,  0.2764, -0.2213,  0.2207,  0.2417, -0.2708, -0.0631,  0.2274,\n",
      "          0.0414,  0.2845],\n",
      "        [ 0.0714, -0.2718,  0.0267,  0.1020, -0.2460, -0.0422,  0.1788,  0.2341,\n",
      "          0.0821,  0.1868],\n",
      "        [-0.1076,  0.1264, -0.0432,  0.1256, -0.0773, -0.0402, -0.1413, -0.0255,\n",
      "          0.2405, -0.1550],\n",
      "        [ 0.3037,  0.2018,  0.1673,  0.0680,  0.2766,  0.2242, -0.2792,  0.1541,\n",
      "         -0.2107,  0.1627],\n",
      "        [ 0.1945, -0.1604,  0.1950,  0.1993,  0.1532, -0.0840, -0.2860,  0.0398,\n",
      "          0.2701,  0.1927],\n",
      "        [-0.0116, -0.1030,  0.0851,  0.2770, -0.2080, -0.2178,  0.1308,  0.1374,\n",
      "          0.1043, -0.1860]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0481, -0.1324, -0.0383, -0.2728, -0.2870, -0.2191, -0.1806,  0.0917,\n",
      "         0.0168,  0.2914], requires_grad=True)\n",
      "Name: custom_layer.linear.weight, Parameter: Parameter containing:\n",
      "tensor([[-0.0512, -0.2315, -0.1962,  0.2606,  0.1264,  0.2654,  0.3156,  0.1635,\n",
      "         -0.3126, -0.2671],\n",
      "        [ 0.2043, -0.0310,  0.0767,  0.0371, -0.1730,  0.2702, -0.1422,  0.2776,\n",
      "         -0.2358, -0.2810],\n",
      "        [-0.0820,  0.0835, -0.1028, -0.0130,  0.1223,  0.3073, -0.3141, -0.0494,\n",
      "          0.2477,  0.1031],\n",
      "        [ 0.0844, -0.2334, -0.1582,  0.0947, -0.0266,  0.2224,  0.1903, -0.1820,\n",
      "         -0.2917,  0.0374],\n",
      "        [-0.1597,  0.2803, -0.0608,  0.2016, -0.2176,  0.1127, -0.1048, -0.1220,\n",
      "          0.2115, -0.0757],\n",
      "        [ 0.1828, -0.1510,  0.1198, -0.0966, -0.0556, -0.1674, -0.3150, -0.2771,\n",
      "         -0.2073, -0.1603],\n",
      "        [-0.1466, -0.0732,  0.2274, -0.2766,  0.1452,  0.0562, -0.0584, -0.2741,\n",
      "         -0.1187, -0.1150],\n",
      "        [-0.0080, -0.1898,  0.3152, -0.0937, -0.2567, -0.2540,  0.2921,  0.3155,\n",
      "          0.0425, -0.2252],\n",
      "        [-0.2077, -0.0283,  0.2923, -0.0017,  0.1291,  0.2094, -0.3106,  0.0599,\n",
      "          0.1387, -0.2339],\n",
      "        [-0.0823,  0.1679, -0.2316, -0.2191,  0.0834,  0.2770, -0.3059,  0.1802,\n",
      "         -0.0250, -0.1676]], requires_grad=True)\n",
      "Name: custom_layer.linear.bias, Parameter: Parameter containing:\n",
      "tensor([-0.2617, -0.1249,  0.2150, -0.1322, -0.2724,  0.0307, -0.2291, -0.2611,\n",
      "         0.0886,  0.0698], requires_grad=True)\n",
      "Name: custom_layer.output_linear.weight, Parameter: Parameter containing:\n",
      "tensor([[-0.2853, -0.0527,  0.2485, -0.1523, -0.3049, -0.1844,  0.2710, -0.0303,\n",
      "         -0.1712,  0.0060],\n",
      "        [-0.0221, -0.2080,  0.2171, -0.3014, -0.3152,  0.1881, -0.0438,  0.1773,\n",
      "         -0.3108,  0.0866],\n",
      "        [ 0.0690, -0.0037,  0.2500,  0.1131,  0.1054, -0.1021,  0.1402, -0.0157,\n",
      "         -0.1164, -0.1559],\n",
      "        [-0.0256,  0.1969, -0.2871,  0.0233, -0.2212,  0.2063,  0.2744,  0.2990,\n",
      "         -0.1197,  0.1330],\n",
      "        [-0.1440, -0.2486,  0.2715,  0.2144,  0.1122, -0.0109, -0.3074, -0.0053,\n",
      "          0.2539, -0.0369],\n",
      "        [-0.0379,  0.2019, -0.2354,  0.0030, -0.0180,  0.1955,  0.1097,  0.2259,\n",
      "         -0.1523, -0.2105],\n",
      "        [-0.0162, -0.2989, -0.2851,  0.0618,  0.1607, -0.0036, -0.2805,  0.1992,\n",
      "         -0.0104,  0.2040],\n",
      "        [ 0.0618, -0.2502,  0.1862, -0.0858,  0.1601, -0.1237,  0.3021, -0.1763,\n",
      "         -0.3003, -0.1073],\n",
      "        [-0.1888, -0.1560, -0.2664,  0.1501,  0.0845, -0.3034,  0.0431,  0.1203,\n",
      "         -0.1746, -0.2261],\n",
      "        [ 0.1737, -0.0655,  0.2354,  0.0158, -0.2733, -0.0051, -0.0231,  0.2888,\n",
      "          0.3006, -0.2592]], requires_grad=True)\n",
      "Name: custom_layer.output_linear.bias, Parameter: Parameter containing:\n",
      "tensor([-0.0115,  0.1020, -0.0740, -0.1966, -0.1085,  0.1823, -0.1160, -0.2882,\n",
      "        -0.1635,  0.2690], requires_grad=True)\n",
      "Name: mlp.layer1.linear.weight, Parameter: Parameter containing:\n",
      "tensor([[ 0.1302,  0.2402, -0.0070, -0.1764, -0.1775,  0.0809,  0.0765, -0.0196,\n",
      "         -0.2668,  0.1533],\n",
      "        [ 0.1973, -0.2901, -0.0259,  0.2050, -0.2955, -0.2479, -0.2976, -0.0320,\n",
      "          0.0361,  0.0473],\n",
      "        [-0.2668, -0.2784,  0.2542, -0.1975,  0.2554,  0.0505, -0.0209, -0.1837,\n",
      "         -0.0728, -0.1919],\n",
      "        [ 0.0208, -0.0286,  0.1006,  0.2432, -0.1412,  0.0596, -0.0318, -0.3124,\n",
      "          0.1493,  0.1978],\n",
      "        [-0.1415,  0.0607, -0.1098, -0.1273,  0.1521, -0.0420, -0.1583, -0.1638,\n",
      "         -0.1073,  0.0573],\n",
      "        [-0.0626, -0.2235,  0.2040, -0.0476,  0.2898,  0.2025, -0.2337, -0.0415,\n",
      "         -0.0769, -0.1134],\n",
      "        [ 0.2215,  0.1620, -0.1351,  0.2454, -0.0771, -0.0151, -0.1461,  0.0714,\n",
      "         -0.0467, -0.0447],\n",
      "        [-0.1132,  0.1729, -0.1047, -0.2792, -0.1742,  0.0510,  0.1895, -0.3109,\n",
      "          0.2130, -0.1721],\n",
      "        [ 0.2088, -0.2823, -0.2499, -0.0350,  0.0368,  0.1138,  0.1809, -0.1991,\n",
      "         -0.1004, -0.1652],\n",
      "        [ 0.2728,  0.0393, -0.1603, -0.2040,  0.0534,  0.1667, -0.0744,  0.2211,\n",
      "          0.1552,  0.2742]], requires_grad=True)\n",
      "Name: mlp.layer1.linear.bias, Parameter: Parameter containing:\n",
      "tensor([ 0.2679,  0.0487, -0.1537, -0.2955,  0.2147,  0.1101, -0.1435, -0.0914,\n",
      "         0.1081, -0.0279], requires_grad=True)\n",
      "Name: mlp.layer2.linear.weight, Parameter: Parameter containing:\n",
      "tensor([[ 0.1437,  0.1081, -0.3018,  0.0952, -0.0660, -0.0199, -0.1425, -0.2202,\n",
      "         -0.1639,  0.1502],\n",
      "        [-0.1696,  0.0174,  0.0193, -0.2285, -0.2654, -0.3080,  0.2612,  0.1214,\n",
      "          0.1513, -0.0394],\n",
      "        [ 0.2628, -0.2084, -0.0075,  0.2829,  0.1070,  0.1120,  0.2598,  0.1543,\n",
      "          0.1575,  0.1027],\n",
      "        [ 0.0674, -0.2835, -0.2711, -0.1552,  0.1338,  0.1502, -0.0402,  0.2665,\n",
      "          0.0099,  0.2007],\n",
      "        [ 0.0101, -0.0400, -0.2807, -0.1645,  0.1160, -0.1847,  0.1357, -0.1934,\n",
      "          0.1193, -0.0986],\n",
      "        [-0.0751,  0.1088,  0.1554, -0.0950, -0.0530,  0.2724,  0.1413,  0.2321,\n",
      "          0.0934,  0.1108],\n",
      "        [ 0.2901,  0.1429,  0.3137, -0.2625, -0.2376,  0.0504, -0.1241, -0.0946,\n",
      "         -0.0974,  0.2988],\n",
      "        [ 0.1286, -0.0193,  0.1617,  0.0574, -0.2665,  0.2347, -0.2016,  0.1542,\n",
      "         -0.0107,  0.1267],\n",
      "        [-0.0136, -0.1946, -0.1494, -0.2989, -0.2272, -0.2406,  0.0776, -0.0866,\n",
      "          0.0849, -0.2181],\n",
      "        [-0.0491,  0.3130,  0.2761, -0.0594,  0.2175, -0.0764, -0.1621, -0.1608,\n",
      "          0.2269, -0.0992]], requires_grad=True)\n",
      "Name: mlp.layer2.linear.bias, Parameter: Parameter containing:\n",
      "tensor([ 0.2708,  0.1400, -0.2533, -0.1351,  0.2679, -0.0405, -0.3028, -0.2625,\n",
      "         0.2684,  0.0485], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "19a0a0e1cbd1c2ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型的保存与加载",
   "id": "2730b0d06856aa81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:06.993633Z",
     "start_time": "2024-05-20T02:17:06.979671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 要保存模型的状态字典，可以使用state_dict()方法，然后使用torch.save()将其写入磁盘。加载模型时，首先创建一个模型实例，然后使用load_state_dict()方法加载保存的参数。\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# 加载模型\n",
    "model = ComplexModel(10,10,10)\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ],
   "id": "d7ad56eef22801c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型的设备移动",
   "id": "e87f398f195079f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.118643Z",
     "start_time": "2024-05-20T02:17:06.994631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用to()方法可以将模型及其所有参数移到GPU或CPU上。如果设备可用，它会尝试将模型移动到GPU上，否则保留在CPU上。将模型移动到GPU（如果可用）：\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)"
   ],
   "id": "3e700618b366d6e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ComplexModel(\n",
       "  (custom_layer): CustomLayer(\n",
       "    (linear): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (output_linear): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (layer1): SimpleLayer(\n",
       "      (linear): Linear(in_features=10, out_features=10, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer2): SimpleLayer(\n",
       "      (linear): Linear(in_features=10, out_features=10, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 自定义层和操作",
   "id": "fa88b2db8873112b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.134063Z",
     "start_time": "2024-05-20T02:17:07.119642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 要创建自定义的神经网络层或操作，可以继承nn.Module，再实现新的前向传播逻辑，包括新的数学函数、正则化、注意力机制等。例如，假设要创建一个自定义的归一化层：\n",
    "class CustomNormalization(nn.Module):\n",
    "   def __init__(self, dim):\n",
    "       super(CustomNormalization, self).__init__()\n",
    "       self.dim = dim\n",
    "\n",
    "   def forward(self, x):\n",
    "       mean = x.mean(dim=self.dim, keepdim=True)\n",
    "       std = x.std(dim=self.dim, keepdim=True)\n",
    "       return (x - mean) / (std + 1e-8)\n",
    "\n",
    "model.add_module('custom_normalization', CustomNormalization(1))\n",
    "print(model)\n",
    "# 首先定义了一个新的层CustomNormalization，它计算输入张量在指定维度上的平均值和标准差，然后对输入进行归一化。这个新层可以像其他任何nn.Module实例一样添加到模型中，并在forward方法中使用。"
   ],
   "id": "60b15df26c97d1e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexModel(\n",
      "  (custom_layer): CustomLayer(\n",
      "    (linear): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (output_linear): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (layer1): SimpleLayer(\n",
      "      (linear): Linear(in_features=10, out_features=10, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (layer2): SimpleLayer(\n",
      "      (linear): Linear(in_features=10, out_features=10, bias=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (custom_normalization): CustomNormalization()\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 优化器",
   "id": "1582e2fc61c4df9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.149495Z",
     "start_time": "2024-05-20T02:17:07.139065Z"
    }
   },
   "cell_type": "code",
   "source": "# 在 PyTorch 中，优化器（Optimizer）是用于更新神经网络模型参数的工具。优化器基于模型参数的梯度信息来调整参数，从而最小化或最大化某个损失函数。PyTorch 提供了多种优化器，包括随机梯度下降（SGD）、Adam、RMSprop 等。\n",
   "id": "6de3b40d4e719798",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SGD随机梯度下降",
   "id": "f5d5e4d9a57fc996"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.165453Z",
     "start_time": "2024-05-20T02:17:07.150492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SGD是最基础的优化算法，它根据梯度的方向逐步调整模型参数，以减少损失函数的值。在PyTorch中，可以通过以下方式创建一个SGD优化器：\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# 其中，model.parameters()用于获取模型的所有可学习参数。lr是学习率，决定了参数更新的步长。momentum是动量项，默认为0，可以加速学习过程并有助于跳出局部最小值。"
   ],
   "id": "b156ad9aedd8fe5c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adam优化器",
   "id": "b265f3fef4ef0d01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.181410Z",
     "start_time": "2024-05-20T02:17:07.166451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adam（Adaptive Moment Estimation）是一种自适应学习率的优化算法，它结合了梯度下降和动量方法。通常不需要手动调整学习率衰减\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "# Adam的默认学习率是0.001，可以指定为lr参数，betas参数指定了动量项的系数，是两个超参数，分别控制了一阶矩和二阶矩估计的衰减率。"
   ],
   "id": "6c685d6f17a3216f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RMSprop优化器",
   "id": "7c4a5b0ea58f8914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.196370Z",
     "start_time": "2024-05-20T02:17:07.182408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RMSprop是一种改进的随机梯度下降算法，它使用滑动平均来估计梯度方差，从而避免了梯度消失和梯度爆炸的问题。在PyTorch中，可以通过以下方式创建一个RMSprop优化器：\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)\n",
    "# RMSprop的默认学习率是0.01，可以指定为lr参数，alpha参数指定了滑动平均的系数，是超参数。"
   ],
   "id": "1458f525c45fbc22",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 损失函数",
   "id": "88a308eb49edb317"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.212327Z",
     "start_time": "2024-05-20T02:17:07.197368Z"
    }
   },
   "cell_type": "code",
   "source": "# 损失函数（Loss Function）在机器学习和深度学习领域扮演着核心角色，它是评估模型预测质量的重要标准。损失函数量化了模型预测值与真实标签之间的偏差，训练过程本质上是通过优化算法（如梯度下降）不断调整模型参数，以最小化这个损失值。PyTorch，作为一个强大的深度学习框架，内置了多种损失函数，以适应不同类型的机器学习任务，主要包括但不限于以下几类：",
   "id": "272902e8e6c10077",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 均方误差损失（Mean Squared Error, MSE）",
   "id": "705af04e51d148c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.227794Z",
     "start_time": "2024-05-20T02:17:07.213325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 均方误差（MSE）损失函数：适用于回归任务，计算预测值与真实标签之间之间差的平方的平均值。\n",
    "\n",
    "# 假设模型输出和真实标签\n",
    "# outputs = model(input)  # 模型的预测输出\n",
    "# targets = labels  # 真实标签\n",
    "\n",
    "# 定义损失函数\n",
    "# mse_loss = nn.MSELoss()\n",
    "\n",
    "# 计算损失\n",
    "# loss = mse_loss(outputs, targets)"
   ],
   "id": "293adc7a27e356ab",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 交叉熵损失（Cross-Entropy）",
   "id": "f4ba12e6757d455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.243751Z",
     "start_time": "2024-05-20T02:17:07.228791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设模型输出为每个类别的概率分布\n",
    "# outputs = model(inputs)\n",
    "\n",
    "# 确保标签是类别索引（而非one-hot编码），对于多分类问题\n",
    "# targets = torch.LongTensor(labels)  # 确保标签是整数\n",
    "\n",
    "# cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss = cross_entropy_loss(outputs, targets)"
   ],
   "id": "4007cc708b78e2ea",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 二元交叉熵损失（Binary Cross-Entropy）",
   "id": "c1d2f7a9162745a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.259708Z",
     "start_time": "2024-05-20T02:17:07.244749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 二元交叉熵损失通常用于二分类问题，其中每个样本属于两个类别之一。\n",
    "\n",
    "# 假设二分类问题，直接输出概率\n",
    "# outputs = model(inputs)  # 输出已经是概率\n",
    "# targets = labels.float()  # 标签转换为float，二分类通常为0或1\n",
    "# \n",
    "# bce_loss = nn.BCELoss()\n",
    "# \n",
    "# loss = bce_loss(outputs, targets)\n",
    "\n",
    "# 如果两类样本数量严重不平衡，可以使用加权二元交叉熵损失（Weighted Binary Cross-Entropy Loss）来调整不同类别的权重，以确保模型在训练时对较少出现的类别给予更多关注。\n",
    "\n",
    "# 假设有两类，其中类0的样本较少\n",
    "# num_samples = [100, 1000]  # 类别0有100个样本，类别1有1000个样本\n",
    "# weights = [1 / num_samples[0], 1 / num_samples[1]]  # 计算类别权重\n",
    "# \n",
    "# # 创建一个加权二元交叉熵损失函数\n",
    "# weighted_bce_loss = nn.BCEWithLogitsLoss(weight=torch.tensor(weights))\n",
    "# \n",
    "# # 假设model是模型，inputs是输入数据，labels是二进制标签（0或1）\n",
    "# outputs = model(inputs)\n",
    "# labels = labels.float()  # 将标签转换为浮点数，因为BCEWithLogitsLoss期望的是概率\n",
    "# \n",
    "# # 计算加权损失\n",
    "# loss = weighted_bce_loss(outputs, labels)"
   ],
   "id": "2a14febd9adcc8c3",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.275666Z",
     "start_time": "2024-05-20T02:17:07.260706Z"
    }
   },
   "cell_type": "code",
   "source": "# 此外还有：K-L 散度损失（Kullback-Leibler Divergence Loss），三元组损失（Triplet Margin Loss）",
   "id": "b35d91271fc71187",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 数据加载与预处理",
   "id": "715919edef048e0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.291461Z",
     "start_time": "2024-05-20T02:17:07.276685Z"
    }
   },
   "cell_type": "code",
   "source": "# 在PyTorch中，数据加载与预处理是构建深度学习模型不可或缺的环节，它确保数据以高效、规范的方式被送入模型进行训练或测试。",
   "id": "d03c1771fa34b853",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据集的定义",
   "id": "d5f3eb37147dc237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.306421Z",
     "start_time": "2024-05-20T02:17:07.292459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中，数据集的定义通常通过创建一个新的类来实现，这个类继承自torch.utils.data.Dataset。这个自定义类需要实现以下两个核心方法：\n",
    "# __len__方法：这个方法返回数据集中的样本数量。它告诉外界调用者数据集中有多少个样本可以用来训练或测试模型。\n",
    "# __getitem__方法：这个方法根据给定的索引返回一个样本数据及其对应的标签（如果有）。它允许按需访问数据集中的任意一个样本，通常包括数据的加载和必要的预处理。"
   ],
   "id": "638bc59dfe9b2efa",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.321381Z",
     "start_time": "2024-05-20T02:17:07.307419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 一个基本的数据集定义示例如下：\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "# 其中，CustomDataset类接收3个参数：data和labels、transform，分别代表数据集中的样本数据和对应的标签及预处理变换。在__init__方法中，将这些参数存储在类的属性中，以便在__getitem__方法中访问。__len__方法返回数据集的长度，即样本数量。__getitem__方法通过索引index获取对应的数据和标签。\n",
    "# 此外，为了增强数据的多样性和模型的泛化能力，可以在数据集类中集成数据预处理逻辑，或者通过传递一个变换对象（如torchvision.transforms中的变换）来动态地对数据进行变换，如旋转、缩放、裁剪等。"
   ],
   "id": "bcae3352189104cf",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据加载器",
   "id": "83cc8d7093e5734d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.337395Z",
     "start_time": "2024-05-20T02:17:07.322379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据加载器（DataLoader）在PyTorch中是一个非常重要的组件，它负责从数据集中高效地加载数据，并为模型训练和验证提供批次（batch）数据。DataLoader类位于torch.utils.data模块中，提供了以下几个关键功能：\n",
    "# 批量加载：它能够将数据集分割成多个小批量（batch），这是深度学习训练过程中的标准做法，有助于提高训练效率和内存利用率。\n",
    "# \n",
    "# 数据混洗：通过设置shuffle=True，可以在每个训练epoch开始前随机打乱数据集的顺序，增加模型训练的随机性，有助于提高模型的泛化能力。\n",
    "# \n",
    "# 多线程加载：通过num_workers参数，可以在后台使用多个线程并发地加载数据，减少数据I/O等待时间，进一步加速训练过程。\n",
    "# \n",
    "# 内存节省：DataLoader通过按需加载数据（即仅在训练过程中需要时才从磁盘加载数据到内存），避免一次性将整个数据集加载到内存中，这对于大规模数据集尤为重要。\n",
    "# \n",
    "# 创建一个DataLoader实例的基本用法如下：\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# from your_dataset_module import YourCustomDataset\n",
    "# \n",
    "# custom_dataset = CustomDataset(data, labels,transform=...)\n",
    "# \n",
    "# # 创建DataLoader实例\n",
    "# data_loader = DataLoader(\n",
    "#     dataset=custom_dataset,  # 数据集实例\n",
    "#     batch_size=32,           # 每个批次的样本数\n",
    "#     shuffle=True,            # 是否在每个epoch开始时打乱数据\n",
    "#     num_workers=4,           # 使用的子进程数，用于数据加载（0表示不使用多线程）\n",
    "#     drop_last=False,         # 如果数据集大小不能被batch_size整除，是否丢弃最后一个不完整的batch\n",
    "# )\n",
    "# \n",
    "# # 使用data_loader在训练循环中迭代获取数据\n",
    "# for inputs, labels in data_loader:\n",
    "#     # 在这里执行模型训练或验证的代码\n",
    "#     pass"
   ],
   "id": "c4db3f6f87db7707",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据预处理与转换",
   "id": "c5cc2917fcac9aae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.353352Z",
     "start_time": "2024-05-20T02:17:07.338393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中，torchvision.transforms模块提供了丰富的预处理和转换功能，以下是一些常用的转换操作：\n",
    "# 「常见的预处理与转换操作：」\n",
    "# Resize：调整图像大小到指定尺寸，例如，transforms.Resize((256, 256))会将图像调整为256x256像素。\n",
    "# \n",
    "# CenterCrop：从图像中心裁剪出指定大小的区域，如transforms.CenterCrop(224)。\n",
    "# \n",
    "# RandomCrop：随机从图像中裁剪出指定大小的区域，增加了数据多样性，有利于模型学习。\n",
    "# \n",
    "# RandomHorizontalFlip：以一定概率水平翻转图像，是常用的数据增强手段之一。\n",
    "# \n",
    "# RandomRotation：随机旋转图像一定角度，进一步增强数据多样性。\n",
    "# \n",
    "# ToTensor：将PIL图像或numpy数组转换为PyTorch的Tensor，并将颜色通道从RGB调整为Tensorflow所期望的格式（HWC -> CHW）。\n",
    "# \n",
    "# Normalize：对图像像素值进行标准化，通常使用特定数据集（如ImageNet）的均值和标准差，例如transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])。\n",
    "# \n",
    "# 「组合变换」\n",
    "# 为了简化应用多个变换的过程，可以使用Compose类将多个变换操作组合在一起，形成一个变换管道，如：\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.RandomCrop((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# \n",
    "# # 将转换应用于数据集\n",
    "# dataset = CustomDataset(data, labels, transform=transform)"
   ],
   "id": "c50a9c8862d7051c",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 学习率调整",
   "id": "ba628709f8e80a04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.369309Z",
     "start_time": "2024-05-20T02:17:07.354351Z"
    }
   },
   "cell_type": "code",
   "source": "# 学习率调整是深度学习模型训练过程中的一个重要环节，它有助于模型在训练过程中找到更好的权重。PyTorch 提供了torch.optim.lr_scheduler模块来实现各种学习率调整策略。",
   "id": "c030d64626271aa5",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## StepLR",
   "id": "718f60fea75a9f59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.385270Z",
     "start_time": "2024-05-20T02:17:07.370308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# StepLR是一种基础且常用的学习率调整策略，它按照预定的周期（通常是按 epoch 计算）来调整学习率。其中，step_size参数指定了衰减发生的时间间隔。比如，如果step_size=5，则学习率每过5个epoch就会调整一次。 gamma参数控制了每次调整时学习率的衰减比例。如果gamma=0.1，这意味着每到达一个step_size，学习率就会乘以0.1，也就是衰减到原来的10%。\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# # 初始化模型和优化器\n",
    "# model = YourModel()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# # 创建 StepLR 调度器\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# # 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     # 训练过程...\n",
    "#     # 在每个epoch结束时调用scheduler的step()方法来更新学习率\n",
    "#     scheduler.step()\n",
    "\n",
    "# 如上示例中，学习率会在每个第5、10、15...个epoch后自动减少为原来的10%，直到训练结束。"
   ],
   "id": "a4e096dbcd865391",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MultiStepLR",
   "id": "47176c10f9d221c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.401227Z",
     "start_time": "2024-05-20T02:17:07.386268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MultiStepLR 与 StepLR 类似，都是基于周期（epoch）来调整学习率，但提供了更灵活的衰减时间点控制。通过 milestones 参数，可以精确指定学习率应该在哪些特定的周期数下降。gamma 参数则决定了每次在这些指定周期学习率下降的比例。\n",
    "# 例如，如果设置milestones=[10, 20, 30]和gamma=0.1，则：\n",
    "# 在训练的第10个epoch结束后，学习率会首次乘以0.1，即减少到原来的10%。\n",
    "# 接着，在第20个epoch后，学习率再次乘以0.1，相对于初始值衰减为原来的1%。\n",
    "# 最后，在第30个epoch后，学习率又一次乘以0.1，最终相对于初始值衰减为原来的0.1%。\n",
    "# 这种方式允许根据训练过程中的性能变化或预期的学习曲线，更加精细地控制学习率的下降时机，有助于模型更好地收敛或避免过拟合。下面是使用 MultiStepLR 的简单示例代码\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import MultiStepLR\n",
    "# \n",
    "# # 初始化模型和优化器\n",
    "# model = YourModel()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# \n",
    "# # 创建 MultiStepLR 调度器\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[10, 20, 30], gamma=0.1)\n",
    "# \n",
    "# # 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     # 训练过程...\n",
    "# \n",
    "#     # 每个epoch结束时调用scheduler的step()方法检查是否需要调整学习率\n",
    "#     scheduler.step()"
   ],
   "id": "cb536ff22689e9ad",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ExponentialLR",
   "id": "251fe5a84a4af1de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.417185Z",
     "start_time": "2024-05-20T02:17:07.402226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ExponentialLR 是一种学习率调整策略，它使学习率按照指数函数的方式逐渐衰减。与 StepLR 和 MultiStepLR 在特定的周期突然改变学习率不同，ExponentialLR 在每个训练步骤（或每个epoch，具体取决于调度器的更新频率）后，都按照一个固定的比率逐渐减少学习率。这对于需要平滑降低学习率，以更细致地探索解空间或在训练后期缓慢逼近最优解的场景非常有用。\n",
    "\n",
    "# 使用 ExponentialLR 的代码示例如下：\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import ExponentialLR\n",
    "# \n",
    "# # 初始化模型和优化器\n",
    "# model = YourModel()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "# \n",
    "# # 创建 ExponentialLR 调度器\n",
    "# # 其中 gamma 参数指定了学习率衰减的速率，例如 gamma=0.9 表示每经过一个调整周期，学习率变为原来的 90%\n",
    "# scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "# \n",
    "# # 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     # 训练过程...\n",
    "# \n",
    "#     # 每个epoch结束时调用scheduler的step()方法更新学习率\n",
    "#     scheduler.step()\n",
    "# 其中，initial_lr即为设定的初始学习率，gamma=0.9 表示每次更新后，学习率都会乘以0.9，因此学习率会以指数形式逐渐减小。"
   ],
   "id": "7325b00807a6e111",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用学习率调整器",
   "id": "2cf76ee36dab70b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.432151Z",
     "start_time": "2024-05-20T02:17:07.419180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中使用学习率调整器（Learning Rate Scheduler）是一个提高模型训练效率和性能的有效策略。下面以StepLR为例展示如何使用学习率调整器。\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# \n",
    "# # 假设的模型定义\n",
    "# class SimpleModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleModel, self).__init__()\n",
    "#         self.linear = nn.Linear(10, 1)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         return self.linear(x)\n",
    "# \n",
    "# # 实例化模型和优化器\n",
    "# model = SimpleModel()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# \n",
    "# # 创建学习率调整器\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # 每5个epoch学习率减半\n",
    "# \n",
    "# # 训练循环\n",
    "# num_epochs = 30\n",
    "# for epoch in range(num_epochs):\n",
    "#     # 假设的训练步骤，这里简化处理\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = F.mse_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# \n",
    "#     # 每个epoch结束时更新学习率\n",
    "#     scheduler.step()\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], LR: {scheduler.get_last_lr()[0]}\")"
   ],
   "id": "fc29d39ccfe14e9a",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 模型评估",
   "id": "3ae693fb385880f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.448102Z",
     "start_time": "2024-05-20T02:17:07.433143Z"
    }
   },
   "cell_type": "code",
   "source": "# 模型评估是机器学习项目中不可或缺的一环，它旨在量化模型在未知数据上的表现，确保模型具有良好的泛化能力。以下是模型评估的关键步骤。",
   "id": "b51b0b2b27ae4888",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 设置模型为评估模式",
   "id": "454a05b7535b6d39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.463067Z",
     "start_time": "2024-05-20T02:17:07.450097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在PyTorch中，通过调用model.eval()方法，模型会被设置为评估模式。这一步骤至关重要，因为它会影响到某些层的行为，比如关闭Dropout层和Batch Normalization层的训练时特有的特性，确保模型的预测是确定性的，并且不会影响模型的内部状态。\n",
    "# model.eval()"
   ],
   "id": "dd6ecbdf32585083",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用验证集或测试集进行推理",
   "id": "114d675643baba70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.479024Z",
     "start_time": "2024-05-20T02:17:07.464065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 遍历验证集或测试集，对输入数据进行前向传播，得到模型的预测输出。\n",
    "# model.eval()\n",
    "# \n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in dataloader:\n",
    "#         outputs = model(inputs)\n",
    "#         # 进行后续处理.."
   ],
   "id": "6bb7d649a53641f4",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 计算性能指标",
   "id": "42a98a328aac39e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 「准确率（Accuracy）」",
   "id": "9caa73a5cbcaa5ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.494981Z",
     "start_time": "2024-05-20T02:17:07.480023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 准确率（Accuracy）是机器学习中最基本且直观的性能评估指标之一，尤其适用于多分类问题。它定义为模型正确分类的样本数占总样本数的比例。\n",
    "# import torch\n",
    "# \n",
    "# # 假设 outputs 和 labels 都是形状为 (batch_size,) 的张量\n",
    "# # 对于多分类问题，outputs 通常包含每个类别的概率，需要获取预测类别\n",
    "# _, predicted = torch.max(outputs.data, 1)\n",
    "# \n",
    "# # 将预测和真实标签转换为相同的数据类型（例如，都转为整数）\n",
    "# if isinstance(predicted, torch.Tensor):\n",
    "#     predicted = predicted.cpu().numpy()\n",
    "# if isinstance(labels, torch.Tensor):\n",
    "#     labels = labels.cpu().numpy()\n",
    "# \n",
    "# # 计算并输出准确率\n",
    "# accuracy = (predicted == labels).sum() / len(labels)\n",
    "# print(f'Accuracy: {accuracy}')"
   ],
   "id": "d69af725316d2b4a",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 「精确度（Precision）」",
   "id": "b4d8411b6443bff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.510939Z",
     "start_time": "2024-05-20T02:17:07.495981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 精确度（Precision）是模型预测为正类的样本中，真正为正类的比例。\n",
    "# from sklearn.metrics import precision_score\n",
    "# \n",
    "# # 假设 predictions 和 true_labels 是形状为 (n_samples,) 的数组\n",
    "# # predictions 是模型的预测结果，true_labels 是对应的真值标签\n",
    "# # 如果是多分类问题，labels 参数是所有类别的列表\n",
    "# \n",
    "# # 二分类问题\n",
    "# precision_binary = precision_score(true_labels, predictions)\n",
    "# \n",
    "# # 多分类问题，宏平均\n",
    "# precision_macro = precision_score(true_labels, predictions, average='macro')\n",
    "# \n",
    "# # 多分类问题，微平均\n",
    "# precision_micro = precision_score(true_labels, predictions, average='micro')\n",
    "# \n",
    "# print(f'Binary Precision: {precision_binary}')\n",
    "# print(f'Macro Average Precision: {precision_macro}')\n",
    "# print(f'Micro Average Precision: {precision_micro}')"
   ],
   "id": "f8fd3238d0ec7584",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 「召回率（Recall）」",
   "id": "bba45c877ab816ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.526899Z",
     "start_time": "2024-05-20T02:17:07.511936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 召回率（Recall），也称为灵敏度或真正率，衡量的是模型识别出的所有正类样本中，正确识别的比例。\n",
    "# from sklearn.metrics import recall_score\n",
    "# \n",
    "# # 假设 predictions 和 true_labels 是形状为 (n_samples,) 的数组\n",
    "# # predictions 是模型的预测结果，true_labels 是对应的真值标签\n",
    "# # 对于多分类问题，labels 参数是所有类别的列表\n",
    "# \n",
    "# # 二分类问题\n",
    "# recall_binary = recall_score(true_labels, predictions)\n",
    "# \n",
    "# # 多分类问题，宏平均\n",
    "# recall_macro = recall_score(true_labels, predictions, average='macro')\n",
    "# \n",
    "# # 多分类问题，微平均\n",
    "# recall_micro = recall_score(true_labels, predictions, average='micro')\n",
    "# \n",
    "# print(f'Binary Recall: {recall_binary}')\n",
    "# print(f'Macro Average Recall: {recall_macro}')\n",
    "# print(f'Micro Average Recall: {recall_micro}')"
   ],
   "id": "732cca1d85b62642",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 「F1分数（F1 Score）」",
   "id": "1cef973837ee90ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.542857Z",
     "start_time": "2024-05-20T02:17:07.527896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# F1分数是精确度（Precision）和召回率（Recall）的调和平均值，旨在提供一个综合评价指标，特别是对于类别不平衡的数据集。\n",
    "# from sklearn.metrics import f1_score\n",
    "# \n",
    "# # 假设 predictions 和 true_labels 是形状为 (n_samples,) 的数组\n",
    "# # predictions 是模型的预测结果，true_labels 是对应的真值标签\n",
    "# # 对于多分类问题，labels 参数是所有类别的列表\n",
    "# \n",
    "# # 二分类问题\n",
    "# f1_binary = f1_score(true_labels, predictions)\n",
    "# \n",
    "# # 多分类问题，宏平均\n",
    "# f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "# \n",
    "# # 多分类问题，微平均\n",
    "# f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "# \n",
    "# print(f'Binary F1 Score: {f1_binary}')\n",
    "# print(f'Macro Average F1 Score: {f1_macro}')\n",
    "# print(f'Micro Average F1 Score: {f1_micro}')"
   ],
   "id": "536e24c927b2c718",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ROC曲线",
   "id": "65a854e24734ce8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T02:17:07.557817Z",
     "start_time": "2024-05-20T02:17:07.543854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ROC曲线（Receiver Operating Characteristic Curve）是一种评估二分类模型性能的方法，特别是在正负样本比例不平衡或者对假阳性（False Positives, FP）和假阴性（False Negatives, FN）的代价不等同的场景下特别有用。ROC曲线通过改变决策阈值，展示了模型在不同阈值下的真正例率（True Positive Rate, TPR）与假正例率（False Positive Rate, FPR）之间的关系。\n",
    "# plt.show()\n",
    "# \n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # 假设 y_true 是二分类的真值标签，y_scores 是模型的得分或概率输出\n",
    "# y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1]  # 真实标签\n",
    "# y_scores = [0.1, 0.4, 0.35, 0.8, 0.6, 0.1, 0.9, 0.7, 0.2, 0.5]  # 模型得分\n",
    "# \n",
    "# # 计算ROC曲线的点\n",
    "# fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "# \n",
    "# # 计算ROC曲线下面积（AUC）\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# \n",
    "# # 绘制ROC曲线\n",
    "# plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')  # 绘制随机猜测的ROC曲线\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")"
   ],
   "id": "8b4c5cef4af8fd7c",
   "outputs": [],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
