{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_jdZ5vHJ4A9"
   },
   "source": [
    "# Task description\n",
    "- Classify the speakers of given features.\n",
    "- Main goal: Learn how to use transformer.\n",
    "- Baselines:\n",
    "  - Easy: Run sample code and know how to use transformer.\n",
    "  - Medium: Know how to adjust parameters of transformer.\n",
    "  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer.\n",
    "  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n",
    "\n",
    "- Other links\n",
    "  - Competiton: [link](https://www.kaggle.com/t/49ea0c385a974db5919ec67299ba2e6b)\n",
    "  - Slide: [link](https://docs.google.com/presentation/d/1LDAW0GGrC9B6D7dlNdYzQL6D60-iKgFr/edit?usp=sharing&ouid=104280564485377739218&rtpof=true&sd=true)\n",
    "  - Data: [link](https://github.com/googly-mingto/ML2023HW4/releases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtKxUzSgXKj3",
    "outputId": "e3fccf00-783c-460d-9b73-3ba612ae78b5"
   },
   "source": [
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\n",
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\n",
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\n",
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\n",
    "\n",
    "!cat Dataset.tar.gz.part* > Dataset.tar.gz\n",
    "!rm Dataset.tar.gz.partaa\n",
    "!rm Dataset.tar.gz.partab\n",
    "!rm Dataset.tar.gz.partac\n",
    "!rm Dataset.tar.gz.partad\n",
    "# unzip the file\n",
    "!tar zxf Dataset.tar.gz\n",
    "!rm Dataset.tar.gz"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6Y1cfpDfpON",
    "outputId": "d8e4863b-9b42-47d4-dd0d-55945d69c1ec"
   },
   "source": [
    "!tar zxf Dataset.tar.gz"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E6burzCXIyuA"
   },
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    设置随机种子，以确保实验的可复现性。\n",
    "    \n",
    "    参数:\n",
    "    seed (int): 随机种子值。\n",
    "    \n",
    "    该函数通过设置numpy、Python内置随机模块、PyTorch的随机种子，以及对CUDA设备的随机种子进行设置，来确保随机数生成的一致性。\n",
    "    此外，还配置了PyTorch的cudnn行为，以确保在使用CUDA时也具有可复现性。\n",
    "    \"\"\"\n",
    "    # 设置numpy的随机种子，保证numpy相关的随机操作可复现\n",
    "    np.random.seed(seed)\n",
    "    # 设置Python内置随机模块的随机种子，使得基于random的随机操作结果一致\n",
    "    random.seed(seed)\n",
    "    # 设置PyTorch的随机种子，确保张量操作等的随机性可控\n",
    "    torch.manual_seed(seed)\n",
    "    # 如果CUDA可用，设置CUDA的随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 禁用cudnn自动寻找最适合当前硬件配置的卷积算法，这有助于结果的复现性\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # 设置cudnn为确定性模式\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 设置随机种子以确保实验可复现性\n",
    "set_seed(87)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7dVbxW2LASN"
   },
   "source": [
    "# Data\n",
    "\n",
    "## Dataset\n",
    "- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n",
    "- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n",
    "- We randomly select 600 speakers from Voxceleb2.\n",
    "- Then preprocess the raw waveforms into mel-spectrograms.\n",
    "\n",
    "- Args:\n",
    "  - data_dir: The path to the data directory.\n",
    "  - metadata_path: The path to the metadata.\n",
    "  - segment_len: The length of audio segment for training.\n",
    "- The architecture of data directory \\\\\n",
    "  - data directory \\\\\n",
    "  |---- metadata.json \\\\\n",
    "  |---- testdata.json \\\\\n",
    "  |---- mapping.json \\\\\n",
    "  |---- uttr-{random string}.pt \\\\\n",
    "\n",
    "- The information in metadata\n",
    "  - \"n_mels\": The dimention of mel-spectrogram.\n",
    "  - \"speakers\": A dictionary.\n",
    "    - Key: speaker ids.\n",
    "    - value: \"feature_path\" and \"mel_len\"\n",
    "\n",
    "\n",
    "For efficiency, we segment the mel-spectrograms into segments in the traing step.\n",
    "\n",
    "为了提高效率，在训练步骤中，我们将梅尔频谱图切分为多个片段"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KpuGxl4CI2pr"
   },
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# 定义一个自定义数据集类myDataset，继承自PyTorch的Dataset基类\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, data_dir, segment_len=128):\n",
    "        # 初始化方法，接收数据目录和段长度作为参数\n",
    "        self.data_dir = data_dir  # 数据目录路径\n",
    "        self.segment_len = segment_len  # 每个数据段的长度，默认为128\n",
    "\n",
    "        # 加载从说话人名称到其对应ID的映射文件\n",
    "        mapping_path = Path(data_dir) / \"mapping.json\"  # 映射文件路径\n",
    "        with mapping_path.open() as f:  # 打开文件\n",
    "            mapping = json.load(f)  # 解析JSON文件内容\n",
    "        self.speaker2id = mapping[\"speaker2id\"]  # 获取映射字典\n",
    "\n",
    "        # 加载训练数据的元数据\n",
    "        metadata_path = Path(data_dir) / \"metadata.json\"  # 元数据文件路径\n",
    "        with open(metadata_path) as f:  # 打开文件\n",
    "            metadata = json.load(f)[\"speakers\"]  # 解析JSON文件内容\n",
    "\n",
    "        # 获取说话人的总数\n",
    "        self.speaker_num = len(metadata.keys())  # 计算键（说话人）的数量\n",
    "\n",
    "        # 初始化一个空列表，用于存储数据\n",
    "        self.data = []\n",
    "\n",
    "        # 遍历所有说话人及其语音片段\n",
    "        for speaker in metadata.keys():\n",
    "            for utterances in metadata[speaker]:  # 遍历说话人的语音片段\n",
    "                # 将特征路径和对应的说话人ID存储到数据列表中\n",
    "                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
    "\n",
    "\n",
    "\n",
    "\t# 返回数据集的总样本数量\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\t# 根据索引获取数据集中的一项数据\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t# 从数据列表中提取特征路径和说话人ID\n",
    "\t\tfeat_path, speaker = self.data[index]\n",
    "\t\t\n",
    "\t\t# 使用torch.load加载预处理过的梅尔频谱图数据\n",
    "\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n",
    "\t\t\n",
    "\t\t# 如果梅尔频谱图的帧数超过设定的segment_len\n",
    "\t\tif len(mel) > self.segment_len:\n",
    "\t\t\t# 随机选择一个起始点，以这个点开始裁剪segment_len长度的片段\n",
    "\t\t\tstart = random.randint(0, len(mel) - self.segment_len)\n",
    "\t\t\t# 截取指定长度的梅尔频谱图片段\n",
    "\t\t\tmel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
    "\t\telse:\n",
    "\t\t\t# 如果原始长度不足，直接转换为FloatTensor\n",
    "\t\t\tmel = torch.FloatTensor(mel)\n",
    "\t\t\t\n",
    "\t\t# 将说话人ID转换为Long类型，以便后续计算损失函数时使用\n",
    "\t\tspeaker = torch.FloatTensor([speaker]).long()\n",
    "\t\t# 返回处理后的梅尔频谱图和说话人ID\n",
    "\t\treturn mel, speaker\n",
    "\n",
    "\t# 获取数据集中说话人的总数\n",
    "\tdef get_speaker_number(self):\n",
    "\t\treturn self.speaker_num\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "668hverTMlGN"
   },
   "source": [
    "## Dataloader\n",
    "- Split dataset into training dataset(90%) and validation dataset(10%).\n",
    "- Create dataloader to iterate the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B7c2gZYoJDRS"
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# 定义一个函数，用于处理批量数据\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    这个函数的作用是将一批数据（batch）进行预处理，使其适合送入模型进行训练。\n",
    "    \n",
    "    参数:\n",
    "    batch: 一个列表，包含多个样本，每个样本由梅尔频谱图和对应的说话人ID组成。\n",
    "    \n",
    "    返回:\n",
    "    - mel: 垂直堆叠并填充后的梅尔频谱图张量，形状为 (batch_size, max_length, 40)。\n",
    "    - speaker: 转换为Long类型的说话人ID张量，形状为 (batch_size,)。\n",
    "    \"\"\"\n",
    "    # 使用zip函数将batch中的梅尔频谱图和说话人ID分开\n",
    "    mel, speaker = zip(*batch)\n",
    "\n",
    "    # 在同一个批次中，我们需要将梅尔频谱图的长度统一，因此使用pad_sequence进行填充\n",
    "    # 参数batch_first=True表示将批次维度放在第一个维度，padding_value=-20表示用非常小的值（log 10^(-20)）进行填充\n",
    "    mel = pad_sequence(mel, batch_first=True, padding_value=-20)  # 填充后梅尔频谱图的形状为 (batch_size, length, 40)\n",
    "\n",
    "    # 将说话人ID转换为Long类型，以便于模型处理\n",
    "    speaker = torch.FloatTensor(speaker).long()\n",
    "\n",
    "    # 返回处理后的梅尔频谱图和说话人ID\n",
    "    return mel, speaker\n",
    "\n",
    "\n",
    "\n",
    "# 定义一个函数，用于生成训练和验证数据加载器\n",
    "def get_dataloader(data_dir, batch_size, n_workers):\n",
    "    \"\"\"\n",
    "    该函数根据给定的参数创建训练和验证数据加载器。\n",
    "    \n",
    "    参数:\n",
    "    - data_dir: 存储数据的目录路径。\n",
    "    - batch_size: 每个批次的样本数量。\n",
    "    - n_workers: 用于数据预处理的子进程数量。\n",
    "    \n",
    "    返回:\n",
    "    - train_loader: 训练数据加载器。\n",
    "    - valid_loader: 验证数据加载器。\n",
    "    - speaker_num: 数据集中说话人的总数。\n",
    "    \"\"\"\n",
    "    # 创建myDataset实例\n",
    "    dataset = myDataset(data_dir)\n",
    "    \n",
    "    # 获取说话人的总数\n",
    "    speaker_num = dataset.get_speaker_number()\n",
    "    \n",
    "    # 将数据集划分为训练集和验证集，比例为9:1\n",
    "    trainlen = int(0.9 * len(dataset))  # 训练集的长度\n",
    "    lengths = [trainlen, len(dataset) - trainlen]  # 训练集和验证集的长度列表\n",
    "    trainset, validset = random_split(dataset, lengths)  # 使用随机分割\n",
    "    \n",
    "    # 创建训练数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        trainset,  # 使用训练集数据\n",
    "        batch_size=batch_size,  # 每个批次的大小\n",
    "        shuffle=True,  # 训练时需要打乱数据顺序\n",
    "        drop_last=True,  # 删除最后一个不足batch_size的小批次\n",
    "        num_workers=n_workers,  # 数据预处理子进程数量\n",
    "        pin_memory=True,  # 使用 pinned memory 提高性能，它避免了数据在CPU和GPU之间传输时因内存页面交换带来的额外延迟，从而加速了数据传输速度\n",
    "        collate_fn=collate_batch,  # 使用之前定义的collate_batch函数\n",
    "    )\n",
    "    \n",
    "    # 创建验证数据加载器\n",
    "    valid_loader = DataLoader(\n",
    "        validset,  # 使用验证集数据\n",
    "        batch_size=batch_size,  # 每个批次的大小\n",
    "        num_workers=n_workers,  # 数据预处理子进程数量\n",
    "        drop_last=True,  # 删除最后一个不足batch_size的小批次\n",
    "        pin_memory=True,  # 使用 pinned memory 提高性能\n",
    "        collate_fn=collate_batch,  # 使用之前定义的collate_batch函数\n",
    "    )\n",
    "    \n",
    "    # 返回训练和验证数据加载器以及说话人的总数\n",
    "    return train_loader, valid_loader, speaker_num\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FOSZYxrMqhc"
   },
   "source": [
    "# Model\n",
    "- TransformerEncoderLayer:\n",
    "  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "  - Parameters:\n",
    "    - d_model: the number of expected features of the input (required).\n",
    "\n",
    "    - nhead: the number of heads of the multiheadattention models (required).\n",
    "\n",
    "    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "\n",
    "    - dropout: the dropout value (default=0.1).\n",
    "\n",
    "    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "- TransformerEncoder:\n",
    "  - TransformerEncoder is a stack of N transformer encoder layers\n",
    "  - Parameters:\n",
    "    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "\n",
    "    - num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "\n",
    "    - norm: the layer normalization component (optional)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iXZ5B0EKJGs8"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义一个分类器模型，用于识别说话人\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "        # 继承自nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        # 输入特征的维度从40转换到d_model\n",
    "        self.prenet = nn.Linear(40, d_model)\n",
    "\n",
    "        # 待完成的任务：将Transformer替换为Conformer\n",
    "        # 参考链接：https://arxiv.org/abs/2005.08100\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, dim_feedforward=256, nhead=2\n",
    "        )\n",
    "\n",
    "        # 从d_model维度的特征映射到说话人数量\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(d_model, n_spks),\n",
    "        )\n",
    "\n",
    "    def forward(self, mels):\n",
    "        \"\"\"\n",
    "        输入参数：\n",
    "            mels: (batch size, length, 40) - 梅尔频谱图的张量\n",
    "\n",
    "        返回：\n",
    "            out: (batch size, n_spks) - 预测的说话人编号张量\n",
    "        \"\"\"\n",
    "        # 应用prenet，将梅尔频谱图转换为d_model维度\n",
    "        out = self.prenet(mels)\n",
    "        \n",
    "        # 转换张量的形状以适应TransformerEncoderLayer\n",
    "        out = out.permute(1, 0, 2)  # (length, batch size, d_model)\n",
    "\n",
    "        # 应用TransformerEncoderLayer\n",
    "        out = self.encoder_layer(out)\n",
    "\n",
    "        # 将形状恢复为(batch size, length, d_model)\n",
    "        out = out.transpose(0, 1)\n",
    "\n",
    "        # 对每个样本的序列进行平均池化\n",
    "        stats = out.mean(dim=1)\n",
    "\n",
    "        # 应用预测层得到说话人编号\n",
    "        out = self.pred_layer(stats)\n",
    "\n",
    "        # 返回预测的说话人编号\n",
    "        return out\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7yX8JinM5Ly"
   },
   "source": [
    "# Learning rate schedule\n",
    "- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n",
    "- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n",
    "- The warmup schedule\n",
    "  - Set learning rate to 0 in the beginning.\n",
    "  - The learning rate increases linearly from 0 to initial learning rate during warmup period."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ykt0N1nVJJi2"
   },
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "# 定义一个函数，用于创建带有预热阶段的余弦退火学习率调度器\n",
    "def get_cosine_schedule_with_warmup(\n",
    "\toptimizer: Optimizer,  # 优化器实例，如Adam或SGD，用于更新模型参数\n",
    "\tnum_warmup_steps: int,  # 预热阶段的步数，学习率线性增长至原设定值\n",
    "\tnum_training_steps: int,  # 总训练步数，用于计算学习率衰减计划\n",
    "\tnum_cycles: float = 0.5,  # 余弦退火周期数，默认0.5意味着半个周期从最大降到0\n",
    "\tlast_epoch: int = -1,  # 上一次训练的epoch数，用于恢复训练状态，默认-1表示从头开始\n",
    "):\n",
    "\n",
    "\t\"\"\"\n",
    "\t创建一个学习率调度器，其学习率遵循余弦函数的规律，从初始学习率逐渐增加到最大值，然后逐渐减小到0。\n",
    "\t在增加阶段有一个预热期，学习率线性增加。\n",
    "\n",
    "\t参数:\n",
    "\t- optimizer: torch.optim.Optimizer类型的优化器，需要调整学习率的优化器。\n",
    "\t- num_warmup_steps: int类型，预热阶段的步数。\n",
    "\t- num_training_steps: int类型，总的训练步数。\n",
    "\t- num_cycles: float类型，可选，默认为0.5，余弦退火周期的数量。\n",
    "\t- last_epoch: int类型，可选，默认为-1，恢复训练时的最后一个epoch。\n",
    "\n",
    "\t返回:\n",
    "\t- LambdaLR: 使用lr_lambda函数作为学习率计算规则的torch.optim.lr_scheduler.LambdaLR对象。\n",
    "\t\"\"\"\n",
    "\n",
    "\t# 定义一个闭包函数lr_lambda，用于计算当前步骤的学习率\n",
    "\tdef lr_lambda(current_step):\n",
    "\t\t# 预热阶段\n",
    "\t\tif current_step < num_warmup_steps:\n",
    "\t\t\t# 学习率线性增加\n",
    "\t\t\treturn current_step / max(1, num_warmup_steps)\n",
    "\t\t# 余弦退火阶段\n",
    "\t\telse:\n",
    "\t\t\t# 计算训练进度\n",
    "\t\t\tprogress = (current_step - num_warmup_steps) / max(\n",
    "\t\t\t\t1, num_training_steps - num_warmup_steps\n",
    "\t\t\t)\n",
    "\t\t\t# 余弦退火公式，num_cycles控制周期次数\n",
    "\t\t\treturn max(\n",
    "\t\t\t\t0.0,\n",
    "\t\t\t\t0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)),\n",
    "\t\t\t)\n",
    "\n",
    "\t# 使用lr_lambda创建LambdaLR调度器，并传入最后一个epoch\n",
    "\treturn LambdaLR(optimizer, lr_lambda, last_epoch)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LN2XkteM_uH"
   },
   "source": [
    "# Model Function\n",
    "- Model forward function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N-rr8529JMz0"
   },
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def model_fn(batch, model, criterion, device):\n",
    "    \"\"\"\n",
    "    该函数负责将一个数据批次（batch）通过模型进行前向传播，并计算损失和准确率。\n",
    "    \n",
    "    参数:\n",
    "    - batch: 包含音频特征（mels）和对应标签（labels）的数据批次。\n",
    "    - model: 训练好的模型，用于对音频特征进行分类。\n",
    "    - criterion: 损失函数，用于衡量模型预测与真实标签之间的差异。\n",
    "    - device: 指定模型和数据应该在哪个设备（如CPU或GPU）上运行。\n",
    "    \n",
    "    返回:\n",
    "    - loss: 该批次数据的平均损失值。\n",
    "    - accuracy: 该批次数据的预测准确率。\n",
    "    \"\"\"\n",
    "\n",
    "    # 解包批次数据，获取音频特征（mels）和标签（labels）\n",
    "    mels, labels = batch\n",
    "\n",
    "    # 将音频特征和标签转移到指定的设备上（如GPU）\n",
    "    mels = mels.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # 通过模型进行前向传播，得到预测结果\n",
    "    outs = model(mels)\n",
    "\n",
    "    # 计算损失（loss），即模型预测结果与真实标签之间的差距\n",
    "    loss = criterion(outs, labels)\n",
    "\n",
    "    # 找出每个样本预测概率最高的说话人ID\n",
    "    preds = outs.argmax(1)\n",
    "\n",
    "    # 计算准确率，即预测正确的样本数占总样本数的比例\n",
    "    accuracy = torch.mean((preds == labels).float())  # .float()将布尔值转换为浮点数以便计算平均值\n",
    "\n",
    "    # 返回损失值和准确率\n",
    "    return loss, accuracy\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwM_xyOtNCI2"
   },
   "source": [
    "# Validate\n",
    "- Calculate accuracy of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YAiv6kpdJRTJ"
   },
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def valid(dataloader, model, criterion, device):\n",
    "\t\"\"\"\n",
    "\t验证模型在验证集上的性能。\n",
    "\n",
    "\t参数:\n",
    "\t- dataloader: DataLoader对象，包含验证集数据。\n",
    "\t- model: 已训练的模型，用于进行预测。\n",
    "\t- criterion: 损失函数，用于计算预测与真实标签之间的差异。\n",
    "\t- device: 设备（如CPU或GPU），用于模型运算。\n",
    "\n",
    "\t返回:\n",
    "\t- avg_accuracy: 验证集上的平均准确率。\n",
    "\t\"\"\"\n",
    "\n",
    "\t# 将模型设置为评估模式，关闭dropout等随机操作\n",
    "\tmodel.eval()\n",
    "\n",
    "\t# 初始化运行损失和运行准确率\n",
    "\trunning_loss = 0.0\n",
    "\trunning_accuracy = 0.0\n",
    "\n",
    "\t# 使用tqdm创建进度条，跟踪验证过程\n",
    "\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n",
    "\n",
    "\t# 遍历验证集的每个批次\n",
    "\tfor i, batch in enumerate(dataloader):\n",
    "\t\t# 在没有梯度计算的环境中运行模型，以节省内存\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# 使用model_fn计算批次的损失和准确率\n",
    "\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n",
    "\t\t\t# 更新运行损失和运行准确率\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\t\trunning_accuracy += accuracy.item()\n",
    "\n",
    "\t\t# 更新进度条\n",
    "\t\tpbar.update(dataloader.batch_size)\n",
    "\t\t# 设置进度条的附加信息，显示当前的平均损失和准确率\n",
    "\t\tpbar.set_postfix(loss=f\"{running_loss / (i+1):.2f}\", accuracy=f\"{running_accuracy / (i+1):.2f}\")\n",
    "\n",
    "\t# 关闭进度条\n",
    "\tpbar.close()\n",
    "\n",
    "\t# 将模型恢复到训练模式\n",
    "\tmodel.train()\n",
    "\n",
    "\t# 返回验证集的平均准确率\n",
    "\treturn running_accuracy / len(dataloader)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6ne9G-eNEdG"
   },
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Usv9s-CuJSG7",
    "outputId": "ad53cea3-459a-40bb-ab51-27b033e785fb"
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "\t\"\"\"\n",
    "\t解析并返回配置参数，用于控制训练流程。\n",
    "\t\"\"\"\n",
    "\tconfig = {\n",
    "\t\t\"data_dir\": \"./Dataset\",  # 数据集的根目录\n",
    "\t\t\"save_path\": \"model.ckpt\",  # 模型权重保存的文件名\n",
    "\t\t\"batch_size\": 32,  # 训练和验证时每个批次的样本数量\n",
    "\t\t\"n_workers\": 8,  # 数据加载时使用的子进程数，用于并行加载数据\n",
    "\t\t\"valid_steps\": 2000,  # 每隔多少训练步进行一次验证\n",
    "\t\t\"warmup_steps\": 1000,  # 学习率预热阶段的步数，从0线性增加到设定值\n",
    "\t\t\"save_steps\": 10000,  # 每隔多少步保存一次模型权重\n",
    "\t\t\"total_steps\": 70000,  # 总的训练步数\n",
    "\t}\n",
    "\treturn config\n",
    "\n",
    "\n",
    "\n",
    "def main(\n",
    "\tdata_dir,  # 数据集目录路径\n",
    "\tsave_path,  # 模型保存路径\n",
    "\tbatch_size,  # 批次大小\n",
    "\tn_workers,  # 数据加载工作线程数\n",
    "\tvalid_steps,  # 验证频率（多少步验证一次）\n",
    "\twarmup_steps,  # 学习率预热步数\n",
    "\ttotal_steps,  # 总训练步数\n",
    "\tsave_steps,  # 保存模型的频率（多少步保存一次）\n",
    "):\n",
    "\t\"\"\"\n",
    "\t主函数，负责整个训练流程的控制。\n",
    "\t包括数据加载、模型初始化、训练循环、验证、模型保存等步骤。\n",
    "\t\"\"\"\n",
    "\n",
    "\t# 设置设备，优先使用GPU\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(f\"[Info]: Use {device} now!\")\n",
    "\n",
    "\t# 加载训练和验证数据集\n",
    "\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n",
    "\ttrain_iterator = iter(train_loader)  # 创建训练数据迭代器\n",
    "\tprint(f\"[Info]: Finish loading data!\", flush=True)\n",
    "\n",
    "\t# 初始化模型、损失函数、优化器和学习率调度器\n",
    "\tmodel = Classifier(n_spks=speaker_num).to(device)  # 创建分类器模型并放置到指定设备\n",
    "\tcriterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，适用于多分类问题\n",
    "\toptimizer = AdamW(model.parameters(), lr=1e-3)  # 使用AdamW优化器，学习率为1e-3\n",
    "\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)  # 余弦退火学习率调度器\n",
    "\tprint(f\"[Info]: Finish creating model!\", flush=True)\n",
    "\n",
    "\t# 初始化最佳准确率和最佳模型参数\n",
    "\tbest_accuracy = -1.0\n",
    "\tbest_state_dict = None\n",
    "\n",
    "\t# 使用tqdm创建训练进度条\n",
    "\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "\t# 主训练循环\n",
    "\tfor step in range(total_steps):\n",
    "\t\t# 获取训练数据\n",
    "\t\ttry:\n",
    "\t\t\tbatch = next(train_iterator)\n",
    "\t\texcept StopIteration:  # 当迭代器耗尽时重新初始化\n",
    "\t\t\ttrain_iterator = iter(train_loader)\n",
    "\t\t\tbatch = next(train_iterator)\n",
    "\n",
    "\t\t# 前向传播、计算损失和准确率\n",
    "\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n",
    "\t\tbatch_loss = loss.item()\n",
    "\t\tbatch_accuracy = accuracy.item()\n",
    "\n",
    "\t\t# 反向传播、优化模型参数、更新学习率\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# 更新进度条信息\n",
    "\t\tpbar.update()\n",
    "\t\tpbar.set_postfix(loss=f\"{batch_loss:.2f}\", accuracy=f\"{batch_accuracy:.2f}\", step=step + 1)\n",
    "\n",
    "\t\t# 验证模型\n",
    "\t\tif (step + 1) % valid_steps == 0:\n",
    "\t\t\tpbar.close()  # 关闭当前进度条\n",
    "\n",
    "\t\t\t# 在验证集上评估模型\n",
    "\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n",
    "\n",
    "\t\t\t# 保存当前最佳模型\n",
    "\t\t\tif valid_accuracy > best_accuracy:\n",
    "\t\t\t\tbest_accuracy = valid_accuracy\n",
    "\t\t\t\tbest_state_dict = model.state_dict()\n",
    "\n",
    "\t\t\t# 重置训练进度条\n",
    "\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "\t\t# 保存模型\n",
    "\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
    "\t\t\t# 保存最佳模型参数到指定路径\n",
    "\t\t\ttorch.save(best_state_dict, save_path)\n",
    "\t\t\t# 在进度条上记录信息\n",
    "\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
    "\n",
    "\tpbar.close()  # 训练结束，关闭进度条\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t\"\"\"\n",
    "\t允许用户通过修改`parse_args`函数中的配置来灵活控制训练流程，而不必直接硬编码这些参数到`main`函数的调用中。\n",
    "\t\"\"\"\n",
    "\tmain(**parse_args())\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLatBYAhNNMx"
   },
   "source": [
    "# Inference\n",
    "\n",
    "## Dataset of inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "efS4pCmAJXJH"
   },
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\t自定义的InferenceDataset类，用于推理阶段的数据加载。\n",
    "\t它继承自torch.utils.data.Dataset，需要实现`__init__`、`__len__`和`__getitem__`方法。\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, data_dir):\n",
    "\t\t\"\"\"\n",
    "\t\t初始化InferenceDataset类。\n",
    "\n",
    "\t\t参数:\n",
    "\t\t- data_dir: str，存储测试数据的目录，包含一个名为\"testdata.json\"的文件。\n",
    "\t\t\"\"\"\n",
    "\t\ttestdata_path = Path(data_dir) / \"testdata.json\"  # 获取测试数据路径\n",
    "\t\tmetadata = json.load(testdata_path.open())  # 读取并加载测试数据的JSON文件\n",
    "\t\tself.data_dir = data_dir  # 保存数据目录\n",
    "\t\tself.data = metadata[\"utterances\"]  # 获取测试数据的“utterances”列表\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"\n",
    "\t\t返回InferenceDataset的长度，即数据集中元素的数量。\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.data)  # 返回“utterances”列表的长度\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t\"\"\"\n",
    "\t\t根据索引获取数据集中的一个元素。\n",
    "\n",
    "\t\t参数:\n",
    "\t\t- index: int，要获取的元素的索引。\n",
    "\n",
    "\t\t返回:\n",
    "\t\t- feat_path: str，对应测试数据的特征路径。\n",
    "\t\t- mel: torch.Tensor，加载的梅尔谱特征。\n",
    "\t\t\"\"\"\n",
    "\t\tutterance = self.data[index]  # 获取索引对应的utterance\n",
    "\t\tfeat_path = utterance[\"feature_path\"]  # 获取特征路径\n",
    "\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))  # 加载梅尔谱特征\n",
    "\n",
    "\t\treturn feat_path, mel  # 返回特征路径和梅尔谱特征\n",
    "\n",
    "\n",
    "def inference_collate_batch(batch):\n",
    "\t\"\"\"\n",
    "\t将一批数据进行堆叠，用于推理阶段的批量处理。\n",
    "\n",
    "\t参数:\n",
    "\t- batch: 一个元组列表，每个元组包含一个特征路径和对应的梅尔谱特征。\n",
    "\n",
    "\t返回:\n",
    "\t- feat_paths: 列表，包含所有样本的特征路径。\n",
    "\t- mels: torch.Tensor，堆叠后的梅尔谱特征。\n",
    "\t\"\"\"\n",
    "\tfeat_paths, mels = zip(*batch)  # 解压batch，将特征路径和梅尔谱分开\n",
    "\treturn feat_paths, torch.stack(mels)  # 返回堆叠后的特征路径列表和梅尔谱张量\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl0WnYwxNK_S"
   },
   "source": [
    "## Main funcrion of Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "9f5699a3a50c4f1bb7e96905ca1588ec",
      "7f8fe74160214f16b07284030ff7ede4",
      "9d897b946bc643ef81759acc06b3a5de",
      "f8556afc1fa4484cb46ac8dd2c28fcbf",
      "6a4f80e1ebda405b9c6f62ae86764819",
      "003e0a32fb8a442b8a804a4d229001f5",
      "800a6c9a5a1e4ace86df3e926f5340ba",
      "bce05aa126924acb810bc0b1d9d0cd7d",
      "6087ec07c64f4d1da4d47f7df2997273",
      "e6b47cf82ad945fcb3be360cb9bfa0ee",
      "babef6aab9f34766bd3c5a9cfc323874"
     ]
    },
    "id": "i8SAbuXEJb2A",
    "outputId": "d5209c5a-c82e-4de3-e2d4-994f482363b6"
   },
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def parse_args():\n",
    "\t\"\"\"\n",
    "\t解析并返回配置参数，用于控制推理流程。\n",
    "\n",
    "\t返回:\n",
    "\t- config: 字典，包含以下键值对：\n",
    "\t\t- data_dir: 数据集目录路径。\n",
    "\t\t- model_path: 模型权重文件路径。\n",
    "\t\t- output_path: 输出结果CSV文件路径。\n",
    "\t\"\"\"\n",
    "\tconfig = {\n",
    "\t\t\"data_dir\": \"./Dataset\",  # 数据集目录\n",
    "\t\t\"model_path\": \"./model.ckpt\",  # 模型权重文件路径\n",
    "\t\t\"output_path\": \"./output.csv\",  # 输出结果CSV文件路径\n",
    "\t}\n",
    "\treturn config\n",
    "\n",
    "\n",
    "def main(\n",
    "\tdata_dir,\n",
    "\tmodel_path,\n",
    "\toutput_path,\n",
    "):\n",
    "\t\"\"\"\n",
    "\t主函数，负责推理流程。\n",
    "\n",
    "\t参数:\n",
    "\t- data_dir: 数据集目录路径。\n",
    "\t- model_path: 模型权重文件路径。\n",
    "\t- output_path: 输出结果CSV文件路径。\n",
    "\t\"\"\"\n",
    "\t# 设置设备，优先使用GPU\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(f\"[Info]: Use {device} now!\")\n",
    "\n",
    "\t# 读取映射文件，获取id到说话人的映射\n",
    "\tmapping_path = Path(data_dir) / \"mapping.json\"\n",
    "\tmapping = json.load(mapping_path.open())\n",
    "\n",
    "\t# 初始化推理数据集和数据加载器\n",
    "\tdataset = InferenceDataset(data_dir)\n",
    "\tdataloader = DataLoader(\n",
    "\t\tdataset,\n",
    "\t\tbatch_size=1,  # 单个样本的批次大小\n",
    "\t\tshuffle=False,  # 不打乱数据顺序\n",
    "\t\tdrop_last=False,  # 保留最后一个不足批次大小的样本\n",
    "\t\tnum_workers=8,  # 数据加载的工作线程数\n",
    "\t\tcollate_fn=inference_collate_batch,  # 自定义的批次合并函数\n",
    "\t)\n",
    "\tprint(f\"[Info]: Finish loading data!\", flush=True)\n",
    "\n",
    "\t# 获取说话人数量\n",
    "\tspeaker_num = len(mapping[\"id2speaker\"])\n",
    "\t# 初始化模型并加载权重，设置为评估模式\n",
    "\tmodel = Classifier(n_spks=speaker_num).to(device)\n",
    "\tmodel.load_state_dict(torch.load(model_path))\n",
    "\tmodel.eval()\n",
    "\tprint(f\"[Info]: Finish creating model!\", flush=True)\n",
    "\n",
    "\t# 初始化输出结果列表\n",
    "\tresults = [[\"Id\", \"Category\"]]  # CSV文件的列标题\n",
    "\n",
    "\t# 进行推理并收集结果\n",
    "\tfor feat_paths, mels in tqdm(dataloader):\n",
    "\t\t# 在无梯度计算环境下运行模型\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tmels = mels.to(device)\n",
    "\t\t\touts = model(mels)\n",
    "\t\t\tpreds = outs.argmax(1).cpu().numpy()  # 获取预测的说话人ID\n",
    "\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n",
    "\t\t\t\t# 将预测结果转换为说话人名称并添加到结果列表\n",
    "\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
    "\n",
    "\t# 将结果写入CSV文件\n",
    "\twith open(output_path, 'w', newline='') as csvfile:\n",
    "\t\twriter = csv.writer(csvfile)\n",
    "\t\twriter.writerows(results)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain(**parse_args())"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "9f5699a3a50c4f1bb7e96905ca1588ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f8fe74160214f16b07284030ff7ede4",
       "IPY_MODEL_9d897b946bc643ef81759acc06b3a5de",
       "IPY_MODEL_f8556afc1fa4484cb46ac8dd2c28fcbf"
      ],
      "layout": "IPY_MODEL_6a4f80e1ebda405b9c6f62ae86764819"
     }
    },
    "7f8fe74160214f16b07284030ff7ede4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003e0a32fb8a442b8a804a4d229001f5",
      "placeholder": "​",
      "style": "IPY_MODEL_800a6c9a5a1e4ace86df3e926f5340ba",
      "value": "100%"
     }
    },
    "9d897b946bc643ef81759acc06b3a5de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bce05aa126924acb810bc0b1d9d0cd7d",
      "max": 8000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6087ec07c64f4d1da4d47f7df2997273",
      "value": 8000
     }
    },
    "f8556afc1fa4484cb46ac8dd2c28fcbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6b47cf82ad945fcb3be360cb9bfa0ee",
      "placeholder": "​",
      "style": "IPY_MODEL_babef6aab9f34766bd3c5a9cfc323874",
      "value": " 8000/8000 [00:30&lt;00:00, 248.33it/s]"
     }
    },
    "6a4f80e1ebda405b9c6f62ae86764819": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "003e0a32fb8a442b8a804a4d229001f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "800a6c9a5a1e4ace86df3e926f5340ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bce05aa126924acb810bc0b1d9d0cd7d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6087ec07c64f4d1da4d47f7df2997273": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6b47cf82ad945fcb3be360cb9bfa0ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "babef6aab9f34766bd3c5a9cfc323874": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
