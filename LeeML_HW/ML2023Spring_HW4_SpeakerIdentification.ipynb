{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_jdZ5vHJ4A9"
   },
   "source": [
    "# Task description\n",
    "- Classify the speakers of given features.\n",
    "- Main goal: Learn how to use transformer.\n",
    "- Baselines:\n",
    "  - Easy: Run sample code and know how to use transformer.\n",
    "  - Medium: Know how to adjust parameters of transformer.\n",
    "  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer.\n",
    "  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n",
    "\n",
    "- Other links\n",
    "  - Competiton: [link](https://www.kaggle.com/t/49ea0c385a974db5919ec67299ba2e6b)\n",
    "  - Slide: [link](https://docs.google.com/presentation/d/1LDAW0GGrC9B6D7dlNdYzQL6D60-iKgFr/edit?usp=sharing&ouid=104280564485377739218&rtpof=true&sd=true)\n",
    "  - Data: [link](https://github.com/googly-mingto/ML2023HW4/releases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtKxUzSgXKj3",
    "outputId": "e3fccf00-783c-460d-9b73-3ba612ae78b5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2024-06-05 14:56:39--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\n",
      "Resolving github.com (github.com)... 140.82.116.3\n",
      "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/7646b36b-6033-4a31-bac4-380c4d21d91e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145639Z&X-Amz-Expires=300&X-Amz-Signature=235e5869fd0e0c813f04d5caccfe71eaf9f7914ddbaad952823721beb7d79977&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partaa&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-06-05 14:56:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/7646b36b-6033-4a31-bac4-380c4d21d91e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145639Z&X-Amz-Expires=300&X-Amz-Signature=235e5869fd0e0c813f04d5caccfe71eaf9f7914ddbaad952823721beb7d79977&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partaa&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1560784333 (1.5G) [application/octet-stream]\n",
      "Saving to: ‘Dataset.tar.gz.partaa’\n",
      "\n",
      "Dataset.tar.gz.part 100%[===================>]   1.45G  68.4MB/s    in 22s     \n",
      "\n",
      "2024-06-05 14:57:02 (67.5 MB/s) - ‘Dataset.tar.gz.partaa’ saved [1560784333/1560784333]\n",
      "\n",
      "--2024-06-05 14:57:02--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\n",
      "Resolving github.com (github.com)... 140.82.116.3\n",
      "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/95b45712-6e2f-4a52-96b1-7d88578345fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145702Z&X-Amz-Expires=300&X-Amz-Signature=fbe33ba4cb002be9b2b963467d417e1a508845ac6be0a831d694e218fdc19283&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partab&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-06-05 14:57:02--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/95b45712-6e2f-4a52-96b1-7d88578345fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145702Z&X-Amz-Expires=300&X-Amz-Signature=fbe33ba4cb002be9b2b963467d417e1a508845ac6be0a831d694e218fdc19283&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partab&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1560784333 (1.5G) [application/octet-stream]\n",
      "Saving to: ‘Dataset.tar.gz.partab’\n",
      "\n",
      "Dataset.tar.gz.part 100%[===================>]   1.45G  75.7MB/s    in 22s     \n",
      "\n",
      "2024-06-05 14:57:25 (66.6 MB/s) - ‘Dataset.tar.gz.partab’ saved [1560784333/1560784333]\n",
      "\n",
      "--2024-06-05 14:57:25--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\n",
      "Resolving github.com (github.com)... 140.82.116.3\n",
      "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0c9d42d3-95b7-4ca4-b57c-ab1a66a5564d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145725Z&X-Amz-Expires=300&X-Amz-Signature=bbcddaad4ad774d427a64a99b83dd381f1c7a267dc999bff26e93cfcac58e193&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partac&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-06-05 14:57:25--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0c9d42d3-95b7-4ca4-b57c-ab1a66a5564d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145725Z&X-Amz-Expires=300&X-Amz-Signature=bbcddaad4ad774d427a64a99b83dd381f1c7a267dc999bff26e93cfcac58e193&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partac&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1560784333 (1.5G) [application/octet-stream]\n",
      "Saving to: ‘Dataset.tar.gz.partac’\n",
      "\n",
      "Dataset.tar.gz.part 100%[===================>]   1.45G  65.9MB/s    in 23s     \n",
      "\n",
      "2024-06-05 14:57:48 (65.1 MB/s) - ‘Dataset.tar.gz.partac’ saved [1560784333/1560784333]\n",
      "\n",
      "--2024-06-05 14:57:48--  https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\n",
      "Resolving github.com (github.com)... 140.82.116.4\n",
      "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0ee11da6-8c96-4463-b084-cea8f95d26e9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145748Z&X-Amz-Expires=300&X-Amz-Signature=3305981bdd80a39bbe5dd4aaa578e29d5616ef6b2cd5eac961ce74069a6a3c9a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partad&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-06-05 14:57:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/606989982/0ee11da6-8c96-4463-b084-cea8f95d26e9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240605%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240605T145748Z&X-Amz-Expires=300&X-Amz-Signature=3305981bdd80a39bbe5dd4aaa578e29d5616ef6b2cd5eac961ce74069a6a3c9a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=606989982&response-content-disposition=attachment%3B%20filename%3DDataset.tar.gz.partad&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1560784336 (1.5G) [application/octet-stream]\n",
      "Saving to: ‘Dataset.tar.gz.partad’\n",
      "\n",
      "Dataset.tar.gz.part 100%[===================>]   1.45G  68.3MB/s    in 22s     \n",
      "\n",
      "2024-06-05 14:58:11 (67.3 MB/s) - ‘Dataset.tar.gz.partad’ saved [1560784336/1560784336]\n",
      "\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.macl'\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\n",
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\n",
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\n",
    "!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\n",
    "\n",
    "!cat Dataset.tar.gz.part* > Dataset.tar.gz\n",
    "!rm Dataset.tar.gz.partaa\n",
    "!rm Dataset.tar.gz.partab\n",
    "!rm Dataset.tar.gz.partac\n",
    "!rm Dataset.tar.gz.partad\n",
    "# unzip the file\n",
    "!tar zxf Dataset.tar.gz\n",
    "!rm Dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6Y1cfpDfpON",
    "outputId": "d8e4863b-9b42-47d4-dd0d-55945d69c1ec"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tar (child): Dataset.tar.gz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!tar zxf Dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E6burzCXIyuA"
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    设置随机种子，以确保实验的可复现性。\n",
    "    \n",
    "    参数:\n",
    "    seed (int): 随机种子值。\n",
    "    \n",
    "    该函数通过设置numpy、Python内置随机模块、PyTorch的随机种子，以及对CUDA设备的随机种子进行设置，来确保随机数生成的一致性。\n",
    "    此外，还配置了PyTorch的cudnn行为，以确保在使用CUDA时也具有可复现性。\n",
    "    \"\"\"\n",
    "    # 设置numpy的随机种子，保证numpy相关的随机操作可复现\n",
    "    np.random.seed(seed)\n",
    "    # 设置Python内置随机模块的随机种子，使得基于random的随机操作结果一致\n",
    "    random.seed(seed)\n",
    "    # 设置PyTorch的随机种子，确保张量操作等的随机性可控\n",
    "    torch.manual_seed(seed)\n",
    "    # 如果CUDA可用，设置CUDA的随机种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 禁用cudnn自动寻找最适合当前硬件配置的卷积算法，这有助于结果的复现性\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # 设置cudnn为确定性模式\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 设置随机种子以确保实验可复现性\n",
    "set_seed(87)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7dVbxW2LASN"
   },
   "source": [
    "# Data\n",
    "\n",
    "## Dataset\n",
    "- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n",
    "- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n",
    "- We randomly select 600 speakers from Voxceleb2.\n",
    "- Then preprocess the raw waveforms into mel-spectrograms.\n",
    "\n",
    "- Args:\n",
    "  - data_dir: The path to the data directory.\n",
    "  - metadata_path: The path to the metadata.\n",
    "  - segment_len: The length of audio segment for training.\n",
    "- The architecture of data directory \\\\\n",
    "  - data directory \\\\\n",
    "  |---- metadata.json \\\\\n",
    "  |---- testdata.json \\\\\n",
    "  |---- mapping.json \\\\\n",
    "  |---- uttr-{random string}.pt \\\\\n",
    "\n",
    "- The information in metadata\n",
    "  - \"n_mels\": The dimention of mel-spectrogram.\n",
    "  - \"speakers\": A dictionary.\n",
    "    - Key: speaker ids.\n",
    "    - value: \"feature_path\" and \"mel_len\"\n",
    "\n",
    "\n",
    "For efficiency, we segment the mel-spectrograms into segments in the traing step.\n",
    "\n",
    "为了提高效率，在训练步骤中，我们将梅尔频谱图切分为多个片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KpuGxl4CI2pr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# 定义一个自定义数据集类myDataset，继承自PyTorch的Dataset基类\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, data_dir, segment_len=128):\n",
    "        # 初始化方法，接收数据目录和段长度作为参数\n",
    "        self.data_dir = data_dir  # 数据目录路径\n",
    "        self.segment_len = segment_len  # 每个数据段的长度，默认为128\n",
    "\n",
    "        # 加载从说话人名称到其对应ID的映射文件\n",
    "        mapping_path = Path(data_dir) / \"mapping.json\"  # 映射文件路径\n",
    "        with mapping_path.open() as f:  # 打开文件\n",
    "            mapping = json.load(f)  # 解析JSON文件内容\n",
    "        self.speaker2id = mapping[\"speaker2id\"]  # 获取映射字典\n",
    "\n",
    "        # 加载训练数据的元数据\n",
    "        metadata_path = Path(data_dir) / \"metadata.json\"  # 元数据文件路径\n",
    "        with open(metadata_path) as f:  # 打开文件\n",
    "            metadata = json.load(f)[\"speakers\"]  # 解析JSON文件内容\n",
    "\n",
    "        # 获取说话人的总数\n",
    "        self.speaker_num = len(metadata.keys())  # 计算键（说话人）的数量\n",
    "\n",
    "        # 初始化一个空列表，用于存储数据\n",
    "        self.data = []\n",
    "\n",
    "        # 遍历所有说话人及其语音片段\n",
    "        for speaker in metadata.keys():\n",
    "            for utterances in metadata[speaker]:  # 遍历说话人的语音片段\n",
    "                # 将特征路径和对应的说话人ID存储到数据列表中\n",
    "                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
    "\n",
    "\n",
    "\n",
    "\t# 返回数据集的总样本数量\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\t# 根据索引获取数据集中的一项数据\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t# 从数据列表中提取特征路径和说话人ID\n",
    "\t\tfeat_path, speaker = self.data[index]\n",
    "\t\t\n",
    "\t\t# 使用torch.load加载预处理过的梅尔频谱图数据\n",
    "\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n",
    "\t\t\n",
    "\t\t# 如果梅尔频谱图的帧数超过设定的segment_len\n",
    "\t\tif len(mel) > self.segment_len:\n",
    "\t\t\t# 随机选择一个起始点，以这个点开始裁剪segment_len长度的片段\n",
    "\t\t\tstart = random.randint(0, len(mel) - self.segment_len)\n",
    "\t\t\t# 截取指定长度的梅尔频谱图片段\n",
    "\t\t\tmel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
    "\t\telse:\n",
    "\t\t\t# 如果原始长度不足，直接转换为FloatTensor\n",
    "\t\t\tmel = torch.FloatTensor(mel)\n",
    "\t\t\t\n",
    "\t\t# 将说话人ID转换为Long类型，以便后续计算损失函数时使用\n",
    "\t\tspeaker = torch.FloatTensor([speaker]).long()\n",
    "\t\t# 返回处理后的梅尔频谱图和说话人ID\n",
    "\t\treturn mel, speaker\n",
    "\n",
    "\t# 获取数据集中说话人的总数\n",
    "\tdef get_speaker_number(self):\n",
    "\t\treturn self.speaker_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "668hverTMlGN"
   },
   "source": [
    "## Dataloader\n",
    "- Split dataset into training dataset(90%) and validation dataset(10%).\n",
    "- Create dataloader to iterate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B7c2gZYoJDRS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# 定义一个函数，用于处理批量数据\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    这个函数的作用是将一批数据（batch）进行预处理，使其适合送入模型进行训练。\n",
    "    \n",
    "    参数:\n",
    "    batch: 一个列表，包含多个样本，每个样本由梅尔频谱图和对应的说话人ID组成。\n",
    "    \n",
    "    返回:\n",
    "    - mel: 垂直堆叠并填充后的梅尔频谱图张量，形状为 (batch_size, max_length, 40)。\n",
    "    - speaker: 转换为Long类型的说话人ID张量，形状为 (batch_size,)。\n",
    "    \"\"\"\n",
    "    # 使用zip函数将batch中的梅尔频谱图和说话人ID分开\n",
    "    mel, speaker = zip(*batch)\n",
    "\n",
    "    # 在同一个批次中，我们需要将梅尔频谱图的长度统一，因此使用pad_sequence进行填充\n",
    "    # 参数batch_first=True表示将批次维度放在第一个维度，padding_value=-20表示用非常小的值（log 10^(-20)）进行填充\n",
    "    mel = pad_sequence(mel, batch_first=True, padding_value=-20)  # 填充后梅尔频谱图的形状为 (batch_size, length, 40)\n",
    "\n",
    "    # 将说话人ID转换为Long类型，以便于模型处理\n",
    "    speaker = torch.FloatTensor(speaker).long()\n",
    "\n",
    "    # 返回处理后的梅尔频谱图和说话人ID\n",
    "    return mel, speaker\n",
    "\n",
    "\n",
    "\n",
    "# 定义一个函数，用于生成训练和验证数据加载器\n",
    "def get_dataloader(data_dir, batch_size, n_workers):\n",
    "    \"\"\"\n",
    "    该函数根据给定的参数创建训练和验证数据加载器。\n",
    "    \n",
    "    参数:\n",
    "    - data_dir: 存储数据的目录路径。\n",
    "    - batch_size: 每个批次的样本数量。\n",
    "    - n_workers: 用于数据预处理的子进程数量。\n",
    "    \n",
    "    返回:\n",
    "    - train_loader: 训练数据加载器。\n",
    "    - valid_loader: 验证数据加载器。\n",
    "    - speaker_num: 数据集中说话人的总数。\n",
    "    \"\"\"\n",
    "    # 创建myDataset实例\n",
    "    dataset = myDataset(data_dir)\n",
    "    \n",
    "    # 获取说话人的总数\n",
    "    speaker_num = dataset.get_speaker_number()\n",
    "    \n",
    "    # 将数据集划分为训练集和验证集，比例为9:1\n",
    "    trainlen = int(0.9 * len(dataset))  # 训练集的长度\n",
    "    lengths = [trainlen, len(dataset) - trainlen]  # 训练集和验证集的长度列表\n",
    "    trainset, validset = random_split(dataset, lengths)  # 使用随机分割\n",
    "    \n",
    "    # 创建训练数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        trainset,  # 使用训练集数据\n",
    "        batch_size=batch_size,  # 每个批次的大小\n",
    "        shuffle=True,  # 训练时需要打乱数据顺序\n",
    "        drop_last=True,  # 删除最后一个不足batch_size的小批次\n",
    "        num_workers=n_workers,  # 数据预处理子进程数量\n",
    "        pin_memory=True,  # 使用 pinned memory 提高性能，它避免了数据在CPU和GPU之间传输时因内存页面交换带来的额外延迟，从而加速了数据传输速度\n",
    "        collate_fn=collate_batch,  # 使用之前定义的collate_batch函数\n",
    "    )\n",
    "    \n",
    "    # 创建验证数据加载器\n",
    "    valid_loader = DataLoader(\n",
    "        validset,  # 使用验证集数据\n",
    "        batch_size=batch_size,  # 每个批次的大小\n",
    "        num_workers=n_workers,  # 数据预处理子进程数量\n",
    "        drop_last=True,  # 删除最后一个不足batch_size的小批次\n",
    "        pin_memory=True,  # 使用 pinned memory 提高性能\n",
    "        collate_fn=collate_batch,  # 使用之前定义的collate_batch函数\n",
    "    )\n",
    "    \n",
    "    # 返回训练和验证数据加载器以及说话人的总数\n",
    "    return train_loader, valid_loader, speaker_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FOSZYxrMqhc"
   },
   "source": [
    "# Model\n",
    "- TransformerEncoderLayer:\n",
    "  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "  - Parameters:\n",
    "    - d_model: the number of expected features of the input (required).\n",
    "\n",
    "    - nhead: the number of heads of the multiheadattention models (required).\n",
    "\n",
    "    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "\n",
    "    - dropout: the dropout value (default=0.1).\n",
    "\n",
    "    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "- TransformerEncoder:\n",
    "  - TransformerEncoder is a stack of N transformer encoder layers\n",
    "  - Parameters:\n",
    "    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "\n",
    "    - num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "\n",
    "    - norm: the layer normalization component (optional)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iXZ5B0EKJGs8",
    "ExecuteTime": {
     "end_time": "2024-06-06T02:14:26.986670Z",
     "start_time": "2024-06-06T02:14:21.978485Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义一个分类器模型，用于识别说话人\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "        # 继承自nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        # 输入特征的维度从40转换到d_model\n",
    "        self.prenet = nn.Linear(40, d_model)\n",
    "\n",
    "        # 待完成的任务：将Transformer替换为Conformer\n",
    "        # 参考链接：https://arxiv.org/abs/2005.08100\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, dim_feedforward=256, nhead=2\n",
    "        )\n",
    "\n",
    "        # 从d_model维度的特征映射到说话人数量\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(d_model, n_spks),\n",
    "        )\n",
    "\n",
    "    def forward(self, mels):\n",
    "        \"\"\"\n",
    "        输入参数：\n",
    "            mels: (batch size, length, 40) - 梅尔频谱图的张量\n",
    "\n",
    "        返回：\n",
    "            out: (batch size, n_spks) - 预测的说话人编号张量\n",
    "        \"\"\"\n",
    "        # 应用prenet，将梅尔频谱图转换为d_model维度\n",
    "        out = self.prenet(mels)\n",
    "        \n",
    "        # 转换张量的形状以适应TransformerEncoderLayer\n",
    "        out = out.permute(1, 0, 2)  # (length, batch size, d_model)\n",
    "\n",
    "        # 应用TransformerEncoderLayer\n",
    "        out = self.encoder_layer(out)\n",
    "\n",
    "        # 将形状恢复为(batch size, length, d_model)\n",
    "        out = out.transpose(0, 1)\n",
    "\n",
    "        # 对每个样本的序列进行平均池化\n",
    "        stats = out.mean(dim=1)\n",
    "\n",
    "        # 应用预测层得到说话人编号\n",
    "        out = self.pred_layer(stats)\n",
    "\n",
    "        # 返回预测的说话人编号\n",
    "        return out\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7yX8JinM5Ly"
   },
   "source": [
    "# Learning rate schedule\n",
    "- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n",
    "- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n",
    "- The warmup schedule\n",
    "  - Set learning rate to 0 in the beginning.\n",
    "  - The learning rate increases linearly from 0 to initial learning rate during warmup period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ykt0N1nVJJi2"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "# 定义一个函数，用于创建带有预热阶段的余弦退火学习率调度器\n",
    "def get_cosine_schedule_with_warmup(\n",
    "\toptimizer: Optimizer,  # 优化器实例，如Adam或SGD，用于更新模型参数\n",
    "\tnum_warmup_steps: int,  # 预热阶段的步数，学习率线性增长至原设定值\n",
    "\tnum_training_steps: int,  # 总训练步数，用于计算学习率衰减计划\n",
    "\tnum_cycles: float = 0.5,  # 余弦退火周期数，默认0.5意味着半个周期从最大降到0\n",
    "\tlast_epoch: int = -1,  # 上一次训练的epoch数，用于恢复训练状态，默认-1表示从头开始\n",
    "):\n",
    "\n",
    "\t\"\"\"\n",
    "\t创建一个学习率调度器，其学习率遵循余弦函数的规律，从初始学习率逐渐增加到最大值，然后逐渐减小到0。\n",
    "\t在增加阶段有一个预热期，学习率线性增加。\n",
    "\n",
    "\t参数:\n",
    "\t- optimizer: torch.optim.Optimizer类型的优化器，需要调整学习率的优化器。\n",
    "\t- num_warmup_steps: int类型，预热阶段的步数。\n",
    "\t- num_training_steps: int类型，总的训练步数。\n",
    "\t- num_cycles: float类型，可选，默认为0.5，余弦退火周期的数量。\n",
    "\t- last_epoch: int类型，可选，默认为-1，恢复训练时的最后一个epoch。\n",
    "\n",
    "\t返回:\n",
    "\t- LambdaLR: 使用lr_lambda函数作为学习率计算规则的torch.optim.lr_scheduler.LambdaLR对象。\n",
    "\t\"\"\"\n",
    "\n",
    "\t# 定义一个闭包函数lr_lambda，用于计算当前步骤的学习率\n",
    "\tdef lr_lambda(current_step):\n",
    "\t\t# 预热阶段\n",
    "\t\tif current_step < num_warmup_steps:\n",
    "\t\t\t# 学习率线性增加\n",
    "\t\t\treturn current_step / max(1, num_warmup_steps)\n",
    "\t\t# 余弦退火阶段\n",
    "\t\telse:\n",
    "\t\t\t# 计算训练进度\n",
    "\t\t\tprogress = (current_step - num_warmup_steps) / max(\n",
    "\t\t\t\t1, num_training_steps - num_warmup_steps\n",
    "\t\t\t)\n",
    "\t\t\t# 余弦退火公式，num_cycles控制周期次数\n",
    "\t\t\treturn max(\n",
    "\t\t\t\t0.0,\n",
    "\t\t\t\t0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)),\n",
    "\t\t\t)\n",
    "\n",
    "\t# 使用lr_lambda创建LambdaLR调度器，并传入最后一个epoch\n",
    "\treturn LambdaLR(optimizer, lr_lambda, last_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LN2XkteM_uH"
   },
   "source": [
    "# Model Function\n",
    "- Model forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N-rr8529JMz0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def model_fn(batch, model, criterion, device):\n",
    "    \"\"\"\n",
    "    该函数负责将一个数据批次（batch）通过模型进行前向传播，并计算损失和准确率。\n",
    "    \n",
    "    参数:\n",
    "    - batch: 包含音频特征（mels）和对应标签（labels）的数据批次。\n",
    "    - model: 训练好的模型，用于对音频特征进行分类。\n",
    "    - criterion: 损失函数，用于衡量模型预测与真实标签之间的差异。\n",
    "    - device: 指定模型和数据应该在哪个设备（如CPU或GPU）上运行。\n",
    "    \n",
    "    返回:\n",
    "    - loss: 该批次数据的平均损失值。\n",
    "    - accuracy: 该批次数据的预测准确率。\n",
    "    \"\"\"\n",
    "\n",
    "    # 解包批次数据，获取音频特征（mels）和标签（labels）\n",
    "    mels, labels = batch\n",
    "\n",
    "    # 将音频特征和标签转移到指定的设备上（如GPU）\n",
    "    mels = mels.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # 通过模型进行前向传播，得到预测结果\n",
    "    outs = model(mels)\n",
    "\n",
    "    # 计算损失（loss），即模型预测结果与真实标签之间的差距\n",
    "    loss = criterion(outs, labels)\n",
    "\n",
    "    # 找出每个样本预测概率最高的说话人ID\n",
    "    preds = outs.argmax(1)\n",
    "\n",
    "    # 计算准确率，即预测正确的样本数占总样本数的比例\n",
    "    accuracy = torch.mean((preds == labels).float())  # .float()将布尔值转换为浮点数以便计算平均值\n",
    "\n",
    "    # 返回损失值和准确率\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwM_xyOtNCI2"
   },
   "source": [
    "# Validate\n",
    "- Calculate accuracy of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YAiv6kpdJRTJ"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def valid(dataloader, model, criterion, device):\n",
    "\t\"\"\"\n",
    "\t验证模型在验证集上的性能。\n",
    "\n",
    "\t参数:\n",
    "\t- dataloader: DataLoader对象，包含验证集数据。\n",
    "\t- model: 已训练的模型，用于进行预测。\n",
    "\t- criterion: 损失函数，用于计算预测与真实标签之间的差异。\n",
    "\t- device: 设备（如CPU或GPU），用于模型运算。\n",
    "\n",
    "\t返回:\n",
    "\t- avg_accuracy: 验证集上的平均准确率。\n",
    "\t\"\"\"\n",
    "\n",
    "\t# 将模型设置为评估模式，关闭dropout等随机操作\n",
    "\tmodel.eval()\n",
    "\n",
    "\t# 初始化运行损失和运行准确率\n",
    "\trunning_loss = 0.0\n",
    "\trunning_accuracy = 0.0\n",
    "\n",
    "\t# 使用tqdm创建进度条，跟踪验证过程\n",
    "\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n",
    "\n",
    "\t# 遍历验证集的每个批次\n",
    "\tfor i, batch in enumerate(dataloader):\n",
    "\t\t# 在没有梯度计算的环境中运行模型，以节省内存\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# 使用model_fn计算批次的损失和准确率\n",
    "\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n",
    "\t\t\t# 更新运行损失和运行准确率\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\t\t\trunning_accuracy += accuracy.item()\n",
    "\n",
    "\t\t# 更新进度条\n",
    "\t\tpbar.update(dataloader.batch_size)\n",
    "\t\t# 设置进度条的附加信息，显示当前的平均损失和准确率\n",
    "\t\tpbar.set_postfix(loss=f\"{running_loss / (i+1):.2f}\", accuracy=f\"{running_accuracy / (i+1):.2f}\")\n",
    "\n",
    "\t# 关闭进度条\n",
    "\tpbar.close()\n",
    "\n",
    "\t# 将模型恢复到训练模式\n",
    "\tmodel.train()\n",
    "\n",
    "\t# 返回验证集的平均准确率\n",
    "\treturn running_accuracy / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6ne9G-eNEdG"
   },
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Usv9s-CuJSG7",
    "outputId": "ad53cea3-459a-40bb-ab51-27b033e785fb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info]: Use cuda now!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info]: Finish loading data!\n",
      "[Info]: Finish creating model!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train:  80% 1593/2000 [01:06<00:17, 23.59 step/s, accuracy=0.00, loss=5.42, step=1593]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Train: 100% 2000/2000 [01:17<00:00, 25.67 step/s, accuracy=0.00, loss=5.34, step=2000]\n",
      "Valid: 100% 5664/5667 [00:06<00:00, 848.34 uttr/s, accuracy=0.03, loss=5.30] \n",
      "Train: 100% 2000/2000 [00:46<00:00, 43.35 step/s, accuracy=0.12, loss=4.53, step=4000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1242.05 uttr/s, accuracy=0.09, loss=4.66]\n",
      "Train: 100% 2000/2000 [00:44<00:00, 44.63 step/s, accuracy=0.19, loss=3.94, step=6000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1476.14 uttr/s, accuracy=0.14, loss=4.27]\n",
      "Train: 100% 2000/2000 [00:48<00:00, 40.90 step/s, accuracy=0.06, loss=4.12, step=8000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1881.09 uttr/s, accuracy=0.17, loss=4.04]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.23 step/s, accuracy=0.19, loss=3.97, step=1e+4]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1814.44 uttr/s, accuracy=0.19, loss=3.86]\n",
      "Train:   0% 8/2000 [00:00<00:51, 39.06 step/s, accuracy=0.22, loss=3.89, step=1e+4]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 10000, best model saved. (accuracy=0.1944)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100% 2000/2000 [00:44<00:00, 44.66 step/s, accuracy=0.25, loss=3.50, step=12000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1243.80 uttr/s, accuracy=0.22, loss=3.74]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 45.89 step/s, accuracy=0.25, loss=3.60, step=14000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1824.28 uttr/s, accuracy=0.25, loss=3.56]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.23 step/s, accuracy=0.19, loss=3.87, step=16000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1595.47 uttr/s, accuracy=0.27, loss=3.45]\n",
      "Train: 100% 2000/2000 [00:45<00:00, 43.73 step/s, accuracy=0.19, loss=3.53, step=18000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1351.30 uttr/s, accuracy=0.29, loss=3.35]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.00 step/s, accuracy=0.34, loss=3.41, step=2e+4]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1708.51 uttr/s, accuracy=0.31, loss=3.23]\n",
      "Train:   0% 9/2000 [00:00<00:45, 43.72 step/s, accuracy=0.50, loss=2.71, step=2e+4]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 20000, best model saved. (accuracy=0.3072)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100% 2000/2000 [00:47<00:00, 42.29 step/s, accuracy=0.56, loss=2.25, step=22000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1866.30 uttr/s, accuracy=0.32, loss=3.14]\n",
      "Train: 100% 2000/2000 [00:45<00:00, 44.34 step/s, accuracy=0.34, loss=3.33, step=24000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1153.90 uttr/s, accuracy=0.34, loss=3.05]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.18 step/s, accuracy=0.41, loss=2.44, step=26000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1878.23 uttr/s, accuracy=0.34, loss=3.03]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.16 step/s, accuracy=0.38, loss=2.92, step=28000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1510.15 uttr/s, accuracy=0.36, loss=2.93]\n",
      "Train: 100% 2000/2000 [00:45<00:00, 44.20 step/s, accuracy=0.34, loss=3.35, step=3e+4]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1180.70 uttr/s, accuracy=0.39, loss=2.84]\n",
      "Train:   1% 11/2000 [00:00<00:35, 55.59 step/s, accuracy=0.50, loss=2.08, step=3e+4]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 30000, best model saved. (accuracy=0.3907)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100% 2000/2000 [00:43<00:00, 45.51 step/s, accuracy=0.62, loss=2.11, step=32000]\n",
      "Valid: 100% 5664/5667 [00:02<00:00, 1927.17 uttr/s, accuracy=0.40, loss=2.80]\n",
      "Train: 100% 2000/2000 [00:47<00:00, 42.23 step/s, accuracy=0.38, loss=2.48, step=34000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1250.28 uttr/s, accuracy=0.41, loss=2.77]\n",
      "Train: 100% 2000/2000 [00:44<00:00, 44.70 step/s, accuracy=0.47, loss=2.37, step=36000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1520.83 uttr/s, accuracy=0.42, loss=2.67]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.21 step/s, accuracy=0.38, loss=2.82, step=38000]\n",
      "Valid: 100% 5664/5667 [00:02<00:00, 1951.88 uttr/s, accuracy=0.44, loss=2.62]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 45.77 step/s, accuracy=0.44, loss=3.26, step=4e+4]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1285.70 uttr/s, accuracy=0.44, loss=2.59]\n",
      "Train:   0% 8/2000 [00:00<00:46, 42.87 step/s, accuracy=0.47, loss=2.35, step=4e+4]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 40000, best model saved. (accuracy=0.4389)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100% 2000/2000 [00:43<00:00, 45.68 step/s, accuracy=0.50, loss=2.15, step=42000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1337.05 uttr/s, accuracy=0.43, loss=2.60]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.08 step/s, accuracy=0.59, loss=1.88, step=44000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1884.81 uttr/s, accuracy=0.45, loss=2.54]\n",
      "Train: 100% 2000/2000 [00:46<00:00, 42.95 step/s, accuracy=0.34, loss=2.93, step=46000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1449.55 uttr/s, accuracy=0.46, loss=2.48]\n",
      "Train: 100% 2000/2000 [00:45<00:00, 43.79 step/s, accuracy=0.59, loss=2.22, step=48000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1440.05 uttr/s, accuracy=0.47, loss=2.44]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 45.71 step/s, accuracy=0.53, loss=2.01, step=5e+4]\n",
      "Valid: 100% 5664/5667 [00:02<00:00, 1909.06 uttr/s, accuracy=0.46, loss=2.44]\n",
      "Train:   1% 11/2000 [00:00<00:29, 66.81 step/s, accuracy=0.59, loss=2.06, step=5e+4]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 50000, best model saved. (accuracy=0.4657)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100% 2000/2000 [00:42<00:00, 47.23 step/s, accuracy=0.50, loss=2.07, step=52000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1593.32 uttr/s, accuracy=0.47, loss=2.42]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 45.69 step/s, accuracy=0.47, loss=2.32, step=54000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1218.82 uttr/s, accuracy=0.47, loss=2.40]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.03 step/s, accuracy=0.44, loss=2.34, step=56000]\n",
      "Valid: 100% 5664/5667 [00:02<00:00, 1891.02 uttr/s, accuracy=0.49, loss=2.38]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 46.12 step/s, accuracy=0.47, loss=2.39, step=58000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1601.93 uttr/s, accuracy=0.48, loss=2.38]\n",
      "Train: 100% 2000/2000 [00:49<00:00, 40.38 step/s, accuracy=0.66, loss=1.93, step=6e+4]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1184.53 uttr/s, accuracy=0.49, loss=2.35]\n",
      "Train:   0% 8/2000 [00:00<00:44, 45.18 step/s, accuracy=0.56, loss=2.56, step=6e+4]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 60000, best model saved. (accuracy=0.4926)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train: 100% 2000/2000 [00:46<00:00, 43.35 step/s, accuracy=0.28, loss=2.68, step=62000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1493.61 uttr/s, accuracy=0.49, loss=2.33]\n",
      "Train: 100% 2000/2000 [00:44<00:00, 45.32 step/s, accuracy=0.50, loss=2.28, step=64000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1887.29 uttr/s, accuracy=0.49, loss=2.36]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 45.62 step/s, accuracy=0.59, loss=2.44, step=66000]\n",
      "Valid: 100% 5664/5667 [00:04<00:00, 1187.45 uttr/s, accuracy=0.48, loss=2.36]\n",
      "Train: 100% 2000/2000 [00:45<00:00, 44.01 step/s, accuracy=0.47, loss=2.60, step=68000]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1837.65 uttr/s, accuracy=0.49, loss=2.35]\n",
      "Train: 100% 2000/2000 [00:43<00:00, 45.84 step/s, accuracy=0.56, loss=1.78, step=7e+4]\n",
      "Valid: 100% 5664/5667 [00:03<00:00, 1861.44 uttr/s, accuracy=0.48, loss=2.37]\n",
      "Train:   0% 0/2000 [00:00<?, ? step/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 70000, best model saved. (accuracy=0.4926)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "\t\"\"\"\n",
    "\t解析并返回配置参数，用于控制训练流程。\n",
    "\t\"\"\"\n",
    "\tconfig = {\n",
    "\t\t\"data_dir\": \"./Dataset\",  # 数据集的根目录\n",
    "\t\t\"save_path\": \"model.ckpt\",  # 模型权重保存的文件名\n",
    "\t\t\"batch_size\": 32,  # 训练和验证时每个批次的样本数量\n",
    "\t\t\"n_workers\": 8,  # 数据加载时使用的子进程数，用于并行加载数据\n",
    "\t\t\"valid_steps\": 2000,  # 每隔多少训练步进行一次验证\n",
    "\t\t\"warmup_steps\": 1000,  # 学习率预热阶段的步数，从0线性增加到设定值\n",
    "\t\t\"save_steps\": 10000,  # 每隔多少步保存一次模型权重\n",
    "\t\t\"total_steps\": 70000,  # 总的训练步数\n",
    "\t}\n",
    "\treturn config\n",
    "\n",
    "\n",
    "\n",
    "def main(\n",
    "\tdata_dir,  # 数据集目录路径\n",
    "\tsave_path,  # 模型保存路径\n",
    "\tbatch_size,  # 批次大小\n",
    "\tn_workers,  # 数据加载工作线程数\n",
    "\tvalid_steps,  # 验证频率（多少步验证一次）\n",
    "\twarmup_steps,  # 学习率预热步数\n",
    "\ttotal_steps,  # 总训练步数\n",
    "\tsave_steps,  # 保存模型的频率（多少步保存一次）\n",
    "):\n",
    "\t\"\"\"\n",
    "\t主函数，负责整个训练流程的控制。\n",
    "\t包括数据加载、模型初始化、训练循环、验证、模型保存等步骤。\n",
    "\t\"\"\"\n",
    "\n",
    "\t# 设置设备，优先使用GPU\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(f\"[Info]: Use {device} now!\")\n",
    "\n",
    "\t# 加载训练和验证数据集\n",
    "\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n",
    "\ttrain_iterator = iter(train_loader)  # 创建训练数据迭代器\n",
    "\tprint(f\"[Info]: Finish loading data!\", flush=True)\n",
    "\n",
    "\t# 初始化模型、损失函数、优化器和学习率调度器\n",
    "\tmodel = Classifier(n_spks=speaker_num).to(device)  # 创建分类器模型并放置到指定设备\n",
    "\tcriterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，适用于多分类问题\n",
    "\toptimizer = AdamW(model.parameters(), lr=1e-3)  # 使用AdamW优化器，学习率为1e-3\n",
    "\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)  # 余弦退火学习率调度器\n",
    "\tprint(f\"[Info]: Finish creating model!\", flush=True)\n",
    "\n",
    "\t# 初始化最佳准确率和最佳模型参数\n",
    "\tbest_accuracy = -1.0\n",
    "\tbest_state_dict = None\n",
    "\n",
    "\t# 使用tqdm创建训练进度条\n",
    "\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "\t# 主训练循环\n",
    "\tfor step in range(total_steps):\n",
    "\t\t# 获取训练数据\n",
    "\t\ttry:\n",
    "\t\t\tbatch = next(train_iterator)\n",
    "\t\texcept StopIteration:  # 当迭代器耗尽时重新初始化\n",
    "\t\t\ttrain_iterator = iter(train_loader)\n",
    "\t\t\tbatch = next(train_iterator)\n",
    "\n",
    "\t\t# 前向传播、计算损失和准确率\n",
    "\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n",
    "\t\tbatch_loss = loss.item()\n",
    "\t\tbatch_accuracy = accuracy.item()\n",
    "\n",
    "\t\t# 反向传播、优化模型参数、更新学习率\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# 更新进度条信息\n",
    "\t\tpbar.update()\n",
    "\t\tpbar.set_postfix(loss=f\"{batch_loss:.2f}\", accuracy=f\"{batch_accuracy:.2f}\", step=step + 1)\n",
    "\n",
    "\t\t# 验证模型\n",
    "\t\tif (step + 1) % valid_steps == 0:\n",
    "\t\t\tpbar.close()  # 关闭当前进度条\n",
    "\n",
    "\t\t\t# 在验证集上评估模型\n",
    "\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n",
    "\n",
    "\t\t\t# 保存当前最佳模型\n",
    "\t\t\tif valid_accuracy > best_accuracy:\n",
    "\t\t\t\tbest_accuracy = valid_accuracy\n",
    "\t\t\t\tbest_state_dict = model.state_dict()\n",
    "\n",
    "\t\t\t# 重置训练进度条\n",
    "\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "\t\t# 保存模型\n",
    "\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
    "\t\t\t# 保存最佳模型参数到指定路径\n",
    "\t\t\ttorch.save(best_state_dict, save_path)\n",
    "\t\t\t# 在进度条上记录信息\n",
    "\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
    "\n",
    "\tpbar.close()  # 训练结束，关闭进度条\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t\"\"\"\n",
    "\t允许用户通过修改`parse_args`函数中的配置来灵活控制训练流程，而不必直接硬编码这些参数到`main`函数的调用中。\n",
    "\t\"\"\"\n",
    "\tmain(**parse_args())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLatBYAhNNMx"
   },
   "source": [
    "# Inference\n",
    "\n",
    "## Dataset of inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "efS4pCmAJXJH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\t自定义的InferenceDataset类，用于推理阶段的数据加载。\n",
    "\t它继承自torch.utils.data.Dataset，需要实现`__init__`、`__len__`和`__getitem__`方法。\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, data_dir):\n",
    "\t\t\"\"\"\n",
    "\t\t初始化InferenceDataset类。\n",
    "\n",
    "\t\t参数:\n",
    "\t\t- data_dir: str，存储测试数据的目录，包含一个名为\"testdata.json\"的文件。\n",
    "\t\t\"\"\"\n",
    "\t\ttestdata_path = Path(data_dir) / \"testdata.json\"  # 获取测试数据路径\n",
    "\t\tmetadata = json.load(testdata_path.open())  # 读取并加载测试数据的JSON文件\n",
    "\t\tself.data_dir = data_dir  # 保存数据目录\n",
    "\t\tself.data = metadata[\"utterances\"]  # 获取测试数据的“utterances”列表\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"\n",
    "\t\t返回InferenceDataset的长度，即数据集中元素的数量。\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.data)  # 返回“utterances”列表的长度\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t\"\"\"\n",
    "\t\t根据索引获取数据集中的一个元素。\n",
    "\n",
    "\t\t参数:\n",
    "\t\t- index: int，要获取的元素的索引。\n",
    "\n",
    "\t\t返回:\n",
    "\t\t- feat_path: str，对应测试数据的特征路径。\n",
    "\t\t- mel: torch.Tensor，加载的梅尔谱特征。\n",
    "\t\t\"\"\"\n",
    "\t\tutterance = self.data[index]  # 获取索引对应的utterance\n",
    "\t\tfeat_path = utterance[\"feature_path\"]  # 获取特征路径\n",
    "\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))  # 加载梅尔谱特征\n",
    "\n",
    "\t\treturn feat_path, mel  # 返回特征路径和梅尔谱特征\n",
    "\n",
    "\n",
    "def inference_collate_batch(batch):\n",
    "\t\"\"\"\n",
    "\t将一批数据进行堆叠，用于推理阶段的批量处理。\n",
    "\n",
    "\t参数:\n",
    "\t- batch: 一个元组列表，每个元组包含一个特征路径和对应的梅尔谱特征。\n",
    "\n",
    "\t返回:\n",
    "\t- feat_paths: 列表，包含所有样本的特征路径。\n",
    "\t- mels: torch.Tensor，堆叠后的梅尔谱特征。\n",
    "\t\"\"\"\n",
    "\tfeat_paths, mels = zip(*batch)  # 解压batch，将特征路径和梅尔谱分开\n",
    "\treturn feat_paths, torch.stack(mels)  # 返回堆叠后的特征路径列表和梅尔谱张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl0WnYwxNK_S"
   },
   "source": [
    "## Main funcrion of Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "9f5699a3a50c4f1bb7e96905ca1588ec",
      "7f8fe74160214f16b07284030ff7ede4",
      "9d897b946bc643ef81759acc06b3a5de",
      "f8556afc1fa4484cb46ac8dd2c28fcbf",
      "6a4f80e1ebda405b9c6f62ae86764819",
      "003e0a32fb8a442b8a804a4d229001f5",
      "800a6c9a5a1e4ace86df3e926f5340ba",
      "bce05aa126924acb810bc0b1d9d0cd7d",
      "6087ec07c64f4d1da4d47f7df2997273",
      "e6b47cf82ad945fcb3be360cb9bfa0ee",
      "babef6aab9f34766bd3c5a9cfc323874"
     ]
    },
    "id": "i8SAbuXEJb2A",
    "outputId": "d5209c5a-c82e-4de3-e2d4-994f482363b6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info]: Use cuda now!\n",
      "[Info]: Finish loading data!\n",
      "[Info]: Finish creating model!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f5699a3a50c4f1bb7e96905ca1588ec"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def parse_args():\n",
    "\t\"\"\"\n",
    "\t解析并返回配置参数，用于控制推理流程。\n",
    "\n",
    "\t返回:\n",
    "\t- config: 字典，包含以下键值对：\n",
    "\t\t- data_dir: 数据集目录路径。\n",
    "\t\t- model_path: 模型权重文件路径。\n",
    "\t\t- output_path: 输出结果CSV文件路径。\n",
    "\t\"\"\"\n",
    "\tconfig = {\n",
    "\t\t\"data_dir\": \"./Dataset\",  # 数据集目录\n",
    "\t\t\"model_path\": \"./model.ckpt\",  # 模型权重文件路径\n",
    "\t\t\"output_path\": \"./output.csv\",  # 输出结果CSV文件路径\n",
    "\t}\n",
    "\treturn config\n",
    "\n",
    "\n",
    "def main(\n",
    "\tdata_dir,\n",
    "\tmodel_path,\n",
    "\toutput_path,\n",
    "):\n",
    "\t\"\"\"\n",
    "\t主函数，负责推理流程。\n",
    "\n",
    "\t参数:\n",
    "\t- data_dir: 数据集目录路径。\n",
    "\t- model_path: 模型权重文件路径。\n",
    "\t- output_path: 输出结果CSV文件路径。\n",
    "\t\"\"\"\n",
    "\t# 设置设备，优先使用GPU\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(f\"[Info]: Use {device} now!\")\n",
    "\n",
    "\t# 读取映射文件，获取id到说话人的映射\n",
    "\tmapping_path = Path(data_dir) / \"mapping.json\"\n",
    "\tmapping = json.load(mapping_path.open())\n",
    "\n",
    "\t# 初始化推理数据集和数据加载器\n",
    "\tdataset = InferenceDataset(data_dir)\n",
    "\tdataloader = DataLoader(\n",
    "\t\tdataset,\n",
    "\t\tbatch_size=1,  # 单个样本的批次大小\n",
    "\t\tshuffle=False,  # 不打乱数据顺序\n",
    "\t\tdrop_last=False,  # 保留最后一个不足批次大小的样本\n",
    "\t\tnum_workers=8,  # 数据加载的工作线程数\n",
    "\t\tcollate_fn=inference_collate_batch,  # 自定义的批次合并函数\n",
    "\t)\n",
    "\tprint(f\"[Info]: Finish loading data!\", flush=True)\n",
    "\n",
    "\t# 获取说话人数量\n",
    "\tspeaker_num = len(mapping[\"id2speaker\"])\n",
    "\t# 初始化模型并加载权重，设置为评估模式\n",
    "\tmodel = Classifier(n_spks=speaker_num).to(device)\n",
    "\tmodel.load_state_dict(torch.load(model_path))\n",
    "\tmodel.eval()\n",
    "\tprint(f\"[Info]: Finish creating model!\", flush=True)\n",
    "\n",
    "\t# 初始化输出结果列表\n",
    "\tresults = [[\"Id\", \"Category\"]]  # CSV文件的列标题\n",
    "\n",
    "\t# 进行推理并收集结果\n",
    "\tfor feat_paths, mels in tqdm(dataloader):\n",
    "\t\t# 在无梯度计算环境下运行模型\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tmels = mels.to(device)\n",
    "\t\t\touts = model(mels)\n",
    "\t\t\tpreds = outs.argmax(1).cpu().numpy()  # 获取预测的说话人ID\n",
    "\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n",
    "\t\t\t\t# 将预测结果转换为说话人名称并添加到结果列表\n",
    "\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
    "\n",
    "\t# 将结果写入CSV文件\n",
    "\twith open(output_path, 'w', newline='') as csvfile:\n",
    "\t\twriter = csv.writer(csvfile)\n",
    "\t\twriter.writerows(results)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain(**parse_args())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "9f5699a3a50c4f1bb7e96905ca1588ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f8fe74160214f16b07284030ff7ede4",
       "IPY_MODEL_9d897b946bc643ef81759acc06b3a5de",
       "IPY_MODEL_f8556afc1fa4484cb46ac8dd2c28fcbf"
      ],
      "layout": "IPY_MODEL_6a4f80e1ebda405b9c6f62ae86764819"
     }
    },
    "7f8fe74160214f16b07284030ff7ede4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003e0a32fb8a442b8a804a4d229001f5",
      "placeholder": "​",
      "style": "IPY_MODEL_800a6c9a5a1e4ace86df3e926f5340ba",
      "value": "100%"
     }
    },
    "9d897b946bc643ef81759acc06b3a5de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bce05aa126924acb810bc0b1d9d0cd7d",
      "max": 8000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6087ec07c64f4d1da4d47f7df2997273",
      "value": 8000
     }
    },
    "f8556afc1fa4484cb46ac8dd2c28fcbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6b47cf82ad945fcb3be360cb9bfa0ee",
      "placeholder": "​",
      "style": "IPY_MODEL_babef6aab9f34766bd3c5a9cfc323874",
      "value": " 8000/8000 [00:30&lt;00:00, 248.33it/s]"
     }
    },
    "6a4f80e1ebda405b9c6f62ae86764819": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "003e0a32fb8a442b8a804a4d229001f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "800a6c9a5a1e4ace86df3e926f5340ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bce05aa126924acb810bc0b1d9d0cd7d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6087ec07c64f4d1da4d47f7df2997273": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6b47cf82ad945fcb3be360cb9bfa0ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "babef6aab9f34766bd3c5a9cfc323874": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
